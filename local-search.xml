<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Python中的变量拷贝</title>
    <link href="/2023/05/08/Python%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E6%8B%B7%E8%B4%9D/"/>
    <url>/2023/05/08/Python%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E6%8B%B7%E8%B4%9D/</url>
    
    <content type="html"><![CDATA[<h1 id="python中的变量拷贝"><a class="markdownIt-Anchor" href="#python中的变量拷贝"></a> Python中的变量拷贝</h1><p>参考文章 <a href="http://iaman.actor/blog/2016/04/17/copy-in-python">http://iaman.actor/blog/2016/04/17/copy-in-python</a></p><blockquote><p><strong>首先直接上结论： —–我们寻常意义的复制就是深复制，即将被复制对象完全再复制一遍作为独立的新个体单独存在。所以改变原有被复制对象不会对已经复制出来的新对象产生影响。<br />—–而浅复制并不会产生一个独立的对象单独存在，他只是将原有的数据块打上一个新标签，所以当其中一个标签被改变的时候，数据块就会发生变化，另一个标签也会随之改变。这就和我们寻常意义上的复制有所不同了。</strong></p></blockquote><p>对于简单的 object，用 shallow copy 和 deep copy 没区别</p><p>复杂的 object， 如 list 中套着 list 的情况，shallow copy 中的 子list，并未从原 object 真的「独立」出来。也就是说，如果你改变原 object 的子 list 中的一个元素，你的 copy 就会跟着一起变。这跟我们直觉上对「复制」的理解不同。</p><p><mark>python中的变量是一个标签，指向内存地址，变量的值是一个对象，想要两个变量分离可以使用deepcopy</mark>：</p><blockquote><p><img src="Python%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E6%8B%B7%E8%B4%9D.assets/v2-04690b0ed49be0031de0f9bef56a01ec_1440w.webp" alt="img" /></p></blockquote><p>看不懂文字没关系我们来看代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> copy<br><span class="hljs-meta">&gt;&gt;&gt; </span>origin = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br><span class="hljs-comment">#origin 里边有三个元素：1， 2，[3, 4]</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>cop1 = copy.copy(origin)<br><span class="hljs-meta">&gt;&gt;&gt; </span>cop2 = copy.deepcopy(origin)<br><span class="hljs-meta">&gt;&gt;&gt; </span>cop1 == cop2<br><span class="hljs-literal">True</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>cop1 <span class="hljs-keyword">is</span> cop2<br><span class="hljs-literal">False</span> <br><span class="hljs-comment">#cop1 和 cop2 看上去相同，但已不再是同一个object</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>origin[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>] = <span class="hljs-string">&quot;hey!&quot;</span> <br><span class="hljs-meta">&gt;&gt;&gt; </span>origin<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, [<span class="hljs-string">&#x27;hey!&#x27;</span>, <span class="hljs-number">4</span>]]<br><span class="hljs-meta">&gt;&gt;&gt; </span>cop1<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, [<span class="hljs-string">&#x27;hey!&#x27;</span>, <span class="hljs-number">4</span>]]<br><span class="hljs-meta">&gt;&gt;&gt; </span>cop2<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br><span class="hljs-comment">#把origin内的子list [3, 4] 改掉了一个元素，观察 cop1 和 cop2123456789101112131415161718</span><br></code></pre></td></tr></table></figure><p>可以看到 cop1，也就是 shallow copy 跟着 origin 改变了。而 cop2 ，也就是 deep copy 并没有变。</p><p>似乎 deep copy 更加符合我们对「复制」的直觉定义: 一旦复制出来了，就应该是独立的了。如果我们想要的是一个字面意义的「copy」，那就直接用 deep_copy 即可。</p><p>那么为什么会有 shallow copy 这样的「假」 copy 存在呢？ 这就是有意思的地方了。</p><h2 id="python的数据存储方式"><a class="markdownIt-Anchor" href="#python的数据存储方式"></a> python的数据存储方式</h2><p>Python 存储变量的方法跟其他 OOP 语言不同。它与其说是把值赋给变量，不如说是给变量建立了一个到具体值的 reference。</p><p>当在 Python 中 a = something 应该理解为给 something 贴上了一个标签 a。当再赋值给 a 的时候，就好象把 a 这个标签从原来的 something 上拿下来，贴到其他对象上，建立新的 reference。 这就解释了一些 Python 中可能遇到的诡异情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">&gt;&gt; a = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>b = a<br><span class="hljs-meta">&gt;&gt;&gt; </span>a = [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>] //赋新的值给 a<br><span class="hljs-meta">&gt;&gt;&gt; </span>a<br>[<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>b<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br><span class="hljs-comment"># a 的值改变后，b 并没有随着 a 变</span><br><br><span class="hljs-meta">&gt;&gt;&gt; </span>a = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>b = a<br><span class="hljs-meta">&gt;&gt;&gt; </span>a[<span class="hljs-number">0</span>], a[<span class="hljs-number">1</span>], a[<span class="hljs-number">2</span>] = <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span> //改变原来 <span class="hljs-built_in">list</span> 中的元素<br><span class="hljs-meta">&gt;&gt;&gt; </span>a<br>[<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>b<br>[<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]<br><span class="hljs-comment"># a 的值改变后，b 随着 a 变了1234567891011121314151617</span><br></code></pre></td></tr></table></figure><p>上面两段代码中，a 的值都发生了变化。区别在于，第一段代码中是直接赋给了 a 新的值（从 [1, 2, 3] 变为 [4, 5, 6]）；而第二段则是把 list 中每个元素分别改变。</p><p>而对 b 的影响则是不同的，一个没有让 b 的值发生改变，另一个变了。怎么用上边的道理来解释这个诡异的不同呢？</p><p>首次把 [1, 2, 3] 看成一个物品。a = [1, 2, 3] 就相当于给这个物品上贴上 a 这个标签。而 b = a 就是给这个物品又贴上了一个 b 的标签。</p><p><img src="Python%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E6%8B%B7%E8%B4%9D.assets/70-16757514703121.gif" alt="这里写图片描述" /><br />第一种情况：</p><p>a = [4, 5, 6] 就相当于把 a 标签从 [1 ,2, 3] 上撕下来，贴到了 [4, 5, 6] 上。</p><p>在这个过程中，[1, 2, 3] 这个物品并没有消失。 b 自始至终都好好的贴在 [1, 2, 3] 上，既然这个 reference 也没有改变过。 b 的值自然不变。<br /><img src="Python%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E6%8B%B7%E8%B4%9D.assets/70-16757514703122.gif" alt="这里写图片描述" /></p><p>第二种情况：</p><p>a[0], a[1], a[2] = 4, 5, 6 则是直接改变了 [1, 2, 3] 这个物品本身。把它内部的每一部分都重新改装了一下。内部改装完毕后，[1, 2, 3] 本身变成了 [4, 5, 6]。</p><p>而在此过程当中，a 和 b 都没有动，他们还贴在那个物品上。因此自然 a b 的值都变成了 [4, 5, 6]。</p><p>搞明白这个之后就要问了，对于一个复杂对象的浅copy，在copy的时候到底发生了什么？<br />再看一段代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> copy<br><span class="hljs-meta">&gt;&gt;&gt; </span>origin = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br><span class="hljs-comment">#origin 里边有三个元素：1， 2，[3, 4]</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>cop1 = copy.copy(origin)<br><span class="hljs-meta">&gt;&gt;&gt; </span>cop2 = copy.deepcopy(origin)<br><span class="hljs-meta">&gt;&gt;&gt; </span>cop1 == cop2<br><span class="hljs-literal">True</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>cop1 <span class="hljs-keyword">is</span> cop2<br><span class="hljs-literal">False</span> <br><span class="hljs-comment">#cop1 和 cop2 看上去相同，但已不再是同一个object</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>origin[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>] = <span class="hljs-string">&quot;hey!&quot;</span> <br><span class="hljs-meta">&gt;&gt;&gt; </span>origin<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, [<span class="hljs-string">&#x27;hey!&#x27;</span>, <span class="hljs-number">4</span>]]<br><span class="hljs-meta">&gt;&gt;&gt; </span>cop1<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, [<span class="hljs-string">&#x27;hey!&#x27;</span>, <span class="hljs-number">4</span>]]<br><span class="hljs-meta">&gt;&gt;&gt; </span>cop2<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br><span class="hljs-comment">#把origin内的子list [3, 4] 改掉了一个元素，观察 cop1 和 cop2123456789101112131415161718</span><br></code></pre></td></tr></table></figure><p>学过docker的人应该对镜像这个概念不陌生，我们可以把镜像的概念套用在copy上面。</p><p>概念图如下：<br /><img src="Python%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E6%8B%B7%E8%B4%9D.assets/70-16757514703123.gif" alt="这里写图片描述" /></p><p>copy对于一个复杂对象的子对象并不会完全复制，什么是复杂对象的子对象呢？就比如序列里的嵌套序列，字典里的嵌套序列等都是复杂对象的子对象。对于子对象，python会把它当作一个公共镜像存储起来，所有对他的复制都被当成一个引用，所以说当其中一个引用将镜像改变了之后另一个引用使用镜像的时候镜像已经被改变了。</p><p>所以说看这里的origin[2]，也就是 [3, 4] 这个 list。根据 shallow copy 的定义，在 cop1[2] 指向的是同一个 list [3, 4]。那么，如果这里我们改变了这个 list，就会导致 origin 和 cop1 同时改变。这就是为什么上边 origin[2][0] = “hey!” 之后，cop1 也随之变成了 [1, 2, [‘hey!’, 4]]。</p><p>而deepcopy概念图如下：<br /><img src="Python%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E6%8B%B7%E8%B4%9D.assets/70-16757514703124.gif" alt="这里写图片描述" /></p><p>deepcopy的时候会将复杂对象的每一层复制一个单独的个体出来。<br />这时候的 origin[2] 和 cop2[2] 虽然值都等于 [3, 4]，但已经不是同一个 list了。即我们寻常意义上的复制。</p><hr /><h2 id="补充"><a class="markdownIt-Anchor" href="#补充"></a> 补充</h2><p>1、非数组变量</p><p>一般的在python中我们将变量a的值赋值给变量b，可以进行如下操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">a = <span class="hljs-number">1</span><br>b = a<br>b += <span class="hljs-number">1</span><br><span class="hljs-built_in">print</span>( <span class="hljs-string">&quot;a =&quot;</span> , a )<br><span class="hljs-built_in">print</span>( <span class="hljs-string">&quot;b =&quot;</span> , b )<br><br></code></pre></td></tr></table></figure><p>得到结果如下图所示：</p><p><img src="Python%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E6%8B%B7%E8%B4%9D.assets/image-20230207143435071.png" alt="image-20230207143435071" /></p><p>从结果中可以看出，我改变b的值，并不会影响a。也就是说对于非数组、列表、字典等类型的变量，直接进行复制，变量b保存的不是地址。</p><p>2、矩阵<br />2.1 使用向量给向量进行赋值<br />   对向量进行赋值操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span>  numpy <span class="hljs-keyword">as</span> np<br>x = np.mat( <span class="hljs-string">&#x27;1 2 3&#x27;</span> )<br>y = x<br>y[<span class="hljs-number">0</span>] += <span class="hljs-number">1</span><br><span class="hljs-built_in">print</span>( <span class="hljs-string">&quot;x = &quot;</span> , x )<br><span class="hljs-built_in">print</span>( <span class="hljs-string">&quot;y = &quot;</span> , y )<br></code></pre></td></tr></table></figure><p>得到结果如下：</p><p><img src="Python%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E6%8B%B7%E8%B4%9D.assets/image-20230207143449189.png" alt="image-20230207143449189" /></p><p>可以看出，改变y的第一个元素的值，x中对应元素值也随之改变，这说明这里保存的是地址。<br />————————————————<br />版权声明：本文为CSDN博主「算法小白，嘤嘤嘤」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br />原文链接：<a href="https://blog.csdn.net/weixin_45307421/article/details/112116172">https://blog.csdn.net/weixin_45307421/article/details/112116172</a></p>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>multi-task learning</title>
    <link href="/2023/05/08/multi-task-learning/"/>
    <url>/2023/05/08/multi-task-learning/</url>
    
    <content type="html"><![CDATA[<h1 id="multi-task-learning-and-beyond-过去现在与未来"><a class="markdownIt-Anchor" href="#multi-task-learning-and-beyond-过去现在与未来"></a> Multi-task Learning and Beyond: 过去，现在与未来</h1><p>近期 Multi-task Learning (MTL) 的研究进展有着众多的科研突破，和许多有趣新方向的探索。这激起了我极大的兴趣来写一篇新文章，尝试概括并总结近期 MTL 的研究进展，并探索未来对于 MTL 研究其他方向的可能。</p><p>这篇文章将顺着我 18 年硕士论文：<a href="https://link.zhihu.com/?target=https%3A//www.imperial.ac.uk/media/imperial-college/faculty-of-engineering/computing/public/1718-pg-projects/LiuS-Transfer-and-Multi-task-Visual-Understanding.pdf">Universal Representations: Towards Multi-Task Learning &amp; Beyond</a> 的大体框架，并加以补充近期新文章的方法，和未来新方向的讨论。</p><p>Disclaimer: 在硕士论文里提及的自己的文章均为当时未发表的 preliminary results，对于任何人想要了解论文里的细节请直接参看发表的 conference 文章。</p><hr /><h2 id="multi-task-learning-的两大研究分支"><a class="markdownIt-Anchor" href="#multi-task-learning-的两大研究分支"></a> Multi-task Learning 的两大研究分支</h2><p>在绝大部分情况下，MTL 的研究可以归类为以下两个方向，一个是 MTL Network 网络设计；另一个是 MTL Loss function 损失函数设计。我们以下对于这两个方向进行详细解读。</p><h3 id="multi-task-learning-network-design-what-to-share-网络设计"><a class="markdownIt-Anchor" href="#multi-task-learning-network-design-what-to-share-网络设计"></a> Multi-task Learning Network Design / What to share? [网络设计]</h3><p>在起初，MTL 的网络设计通常可以列为两种情况：Hard parameter sharing 和 soft parameter sharing。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-5ae0af56b0b18ee348eed50bbd313611_720w.webp" alt="img" /></p><p>Hard-parameter sharing – 现在是几乎所有做 MTL 不可缺少的 baseline 之一，就是将整个 backbone 网络作为 shared network 来 encode 任务信息，在最后一层网络 split 成几个 task-specific decoders 做 prediction。Hard-parameter sharing 是网络设计参数数量 parameter space 的 (并不严格的，假设不考虑用 network pruning) lower bound，由此作为判断新设计网络对于 efficiency v.s. accuracy 平衡的重要参考对象。</p><p>Soft-parameter sharing – 可以看做是 hard-parameter sharing 的另外一个极端，并不常见于现在 MTL 网络设计的比较。在 soft-parameter sharing 中，每一个任务都有其相同大小的 backbone network 作为 parameter space。我们对于其 parameter space 加于特定的 constraint 可以是 sparsity, 或 gradient similarity, 或 LASSO penalty 来 softly* constrain 不同任务网络的 representation space。假设我们不对于 parameter space 加以任何 constraint，那么 soft-parameter sharing 将塌缩成 single task learning。</p><p>任一 MTL 网络设计可以看做是找 hard 和 soft parameter sharing 的平衡点：1. 如何网络设计可以小巧轻便。2. 如何网络设计可以最大幅度的让不同任务去共享信息。</p><p>MTL network design is all about sharing.</p><ul><li><strong>Cross-Stitch Network</strong></li></ul><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1604.03539">Cross-Stitch Network</a> 是过去几年内比较经典的网络设计，也已常用于各类 MTL 研究的baseline 之一。其核心思想是将每个独立的 task-specific 网络使用 learnable parameters (cross-stitch units) 以 linear combination 的方式连接其中不同任务的 convolutional blocks。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-55e3d7d966ee8aa93a40fb3d35544daf_720w.webp" alt="img" /></p><p>Visualisation of Cross-Stitch Network</p><p>对于任务 A 与 B，每个 convolutional block 输出层 xA,Bi,j ，我们将计算：</p><p>[x<sub>Aijx</sub>Bij]=[ΛAAΛABΛBAΛBB][xAijxBij].</p><p>通过这样的运算，下一个 convolutional block 的输入层则为 x~A,Bi,j .</p><p>启发于 Cross-stitch 的设计，<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.08297">NDDR-CNN</a> 也有类似的思路。然而不同的是，对于中间层的 convolutional block 的信息融合，他们采用了 concatenate 并通过 [1 x 1] 的 convolutional layer 来 reduce dimensionality。 这样的设计使得每个任务的 channel 都可以与其他不同 index 的 channel 交融信息，而规避了原始 Cross-stitch 只能 infuse 相同 channel 信息的局限性。当 NDDR 的 convolutional layer weights 的 non-diagonal elements 是 0 的话， NDDR-CNN 则数学上等价于 Cross-Stich Network。</p><p>Cross-Stitch Network 和 NDDR-CNN 的最大弱势就是对于每个任务都需要一个新的网络，以此整个 parameter space 会对于任务的数量增加而线性增加，因此并不 efficient。</p><ul><li><strong>Multi-task Attention Network</strong></li></ul><p>基于 Cross-stitch Network efficiency 的缺点，我后续提出了 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.10704">Multi-task Attention Network (MTAN)</a> 让网络设计更加小巧轻便，整个网络的 parameter space 对于任务数量的增加以 sub-linearly 的方式增加。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-8ca3da13f00bca171cd9ffab407c411a_720w.webp" alt="img" /></p><p>Visualisation of Multi-task Attention Network</p><p>MTAN 的核心思想是，assume 在 shared network encode 得到 general representation 信息之后，我们只需要少量的参数来 refine task-shared representation into task-specific representation, 就可以对于任意任务得到一个很好的 representation. 因此整个网络只需要增加少量的 task-specific attention module，两层 [1 x 1] conv layer，作为额外的 parameter space 来 attend 到 task-shared represenation。整个模型的参数相对于 Cross-Stitch Network 来说则大量减少。</p><ul><li><strong>AdaShare</strong><br /><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.12423">AdaShare</a> 则更是将 MTL 网络设计的 efficiency 做到的极致。与其增加额外的 conv layer 来 refine task-shared representation，AdaShare 将单个 backbone 网络看做 representation 的整体，通过 differentiable task-specific policy 来决定对于任何一个 task，否用去更新或者利用这个网络的 block 的 representation。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-3220fc60f78df10eedecdbcae99ccf3b_720w.webp" alt="img" /></p><p>Visualisation of AdaShare</p><p>由于整个网络是应用于所有任务的 representation，因此 network parameter space 是 agnostic 于任务数量，永远为常数，等价于 hard-parameter sharing。而搭接的 task-specific policy 是利用 gumbel-softmax 对于每一个 conv block 来 categorical sampling “select” 或者 “skip” 两种 policy，因为整个 MTL 的网络设计也因此会随着任务的不同而变化，类似于最近大火的 Neural Architecture Search 的思想。</p><ul><li><strong>MTL + NAS</strong></li></ul><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2003.14058">MTL-NAS</a> 则是将 MTL 和 NAS 结合的另外一个例子。他搭载于 NDDR 的核心思想，将其拓展到任意 block 的交融，因此网络搜索于如何将不同 task 的不同 block 交融来获得最好的 performance。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-3df59b836e13553933207e2173807bfe_720w.webp" alt="img" /></p><p>Visualisation of MTL-NAS</p><p>我个人更偏向 Adashare 的搜索方式，在单个网络里逐层搜索，这样的 task-specific representation 已经足够好过将每一个 task 定义成新网络的结果。由此， MTL-NAS 也躲不掉网络参数线性增加的特点，不过对于 MTL 网络设计提供了新思路。</p><p>MTL + NAS 和传统的 single-task NAS 会有着不同需求，和训练方式。</p><ol><li>MTL+NAS 并不适合用 NAS 里最常见的 two-stage training 方式：以 validation performance 作为 supervision 来 update architecture 参数，得到 converged architecture 后再 re-train 整个网络。因为 MTL 的交融信息具备 training-adaptive 的性质， 因此 fix 网络结构后，这样的 training-adaptive 信息会丢失，得到的 performance 会低于边搜边收敛的 one-stage 方式。换句话说，训练中的 oscillation 和 feature fusion 对于 MTL 网络是更重要的，而在 single task learning 中，并没有 feature fusion 这个概念。这间接导致了 NAS 训练方式的需求不同。</li><li>MTL+NAS is task-specific. 在 NAS 训练中，要是 dataset 的 complexity 过大，有时候我们会采用 proxy task 的方式来加快训练速度。最常见的情况则是用 CIFAR-10 作为 proxy dataset 来搜好的网络结构，应用于过大的 ImageNet dataset。而这一方式并不适用于 MTL，因为对于任一任务，或者几个任务的 pair，他们所需要的 feature 信息和任务特性并不同，因此无法通过 proxy task 的方式来加速训练。每一组任务的网络都是独特和唯一的。</li></ol><p>我相信在未来 MTL 网络设计的研究中，我们会得到更具备 interpretable/human-understandable 的网络特性，能够理解任务与任务之间的相关性，和复杂性。再通过得到的任务相关性，我们可以作为一个很好 prior knowledge 去 initialise 一个更好的起始网络，而由此得到一个更优秀的模型，一种良性循环。</p><p>A Better Task Relationship⟺A Better Multi-task Architecture</p><hr /><h3 id="multi-task-learning-loss-function-design-how-to-learn-损失函数设计与梯度优化"><a class="markdownIt-Anchor" href="#multi-task-learning-loss-function-design-how-to-learn-损失函数设计与梯度优化"></a> Multi-task Learning Loss Function Design / How to learn? [损失函数设计与梯度优化]</h3><p>平行于网络设计，另外一个较为热门的方向是 MTL 的 loss function design, 或者理解为如何去更好得 update 网络里的 task-specific gradients。</p><p>对于任意 task i, 我们有损失函数: L=∑iαiLi , 其中 αi 为 task-specific learning parameters. 那么，我们需要找到一组很好的 αi 来 optimise 所有 task i 的 performance Li 。 其中最为简单且直接的方式则为 equal weighting: αi=1 , 也就是默认每一个 task 对于 representation 的 contribution 是相同的。</p><ul><li><strong>Weight Uncertainty</strong></li></ul><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.07115">Weight Uncertainty</a> 是最早几篇研究 MTL loss function design 的文章之一。这篇文章 assume 在每个 model 里存在一种 data-agnostic task-dependent uncertainty 称之为 Homoscedastic uncertainty。（这种说法其实非常的古怪，只有剑桥组喜欢这么称呼。）而通过 maximise log -likelihood of the model prediction uncertainty 可以来 balance MTL training。这里的 likelihood (通常 parameterised by Gaussian) 可以看做是 relative confidence between tasks。</p><p>对于任何 model prediction y, 我们定义 Gaussian likelihood of model: p(y|fw(x))=N(fw(x),σ2) 其中这里的 σ 为 learnable noise scalar (Gaussian variance)，那么我们需要 maximise:</p><p>log⁡p(y|fw(x))∝−12σ2‖y−fw(x)‖2−log⁡σ,</p><p>由此我们可以得到新定义的 loss function：</p><p>L=∑i12σi2Li+log⁡σi.</p><p>最后推导的公式非常简洁，也因此用在很多 MTL benchmark 里。</p><p>然而这篇文章有着非常大的争议，其中最著名的一点是作者对于如此简单的公式却一直拒绝开源，并且无视大量其他 researchers 的邮件对于 implementation 的细节询问，惹怒了不少同行（包括我）。此外，weight uncertainty 非常依赖于 optimiser 的选择，在我个人实验里，我发现有且仅有 ADAM optimiser 可以让 weight uncertainty 正确收敛，而在其他 optimiser 上 weight uncertainty 没有任何收敛趋势。<a href="https://link.zhihu.com/?target=https%3A//piccolboni.info/2018/03/a-simple-loss-function-for-multi-task-learning-with-keras-implementation.html">这篇博客</a> 则更为指出，这个 weight uncertainty 公式可以直接得到 closed-form solution：当 learnable σ is minimised, 整个 loss function 将转化成 geometric mean of task losses，因此再次对于这里 uncertainty assumption 可行性提出了质疑。</p><ul><li><strong>GradNorm</strong></li></ul><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.02257">GradNorm</a> 则为另外一篇最早期做 MTL loss function 的文章。 GradNorm 的实现是通过计算 inverse training rate: L~(t)=L(t)/L(0) 下降速率，作为 indicator 来平衡不同任务之间的梯度更新。</p><p>我们定义 GW(t) 为 W 参数在 weighted multi-task loss 在 t step 上计算到梯度的 L2-norm; mean task gradient 为 G¯W(t)=E[GiW(t)]; relative inverse training rate 为 ri(t)=L<sub>i(t)/E[L</sub>i(t)] 。 GradNorm 通过以下 objective 来更新 task-specific weighting:</p><p>|GiW(t)−G¯W(t)⋅ri(t)α|</p><p>其中 G¯W(t)⋅ri(t)α 则为理想的梯度 L2-norm (作为 constant), 来调整 task-specific weighting. α 作为一个平衡超参， α 越大则 task-specific weighting 越平衡。由于每次计算 GW(t) 需要对所有 task 在每个 layer 进行 backprop，因此非常 computational expensive。由此，作者就以计算最后一层的 shared layer 作为 approximation 来加快训练速度。</p><ul><li><strong>Dynamic Weight Average</strong></li></ul><p>由于 GradNorm 在计算 task-specific weighting 上需要运算两次 backprop 因此在 implementation 上非常复杂。我后续提出了一个简单的方法，只通过计算 loss 的 relative descending rate 来计算 task weighting:</p><p>αk(t):=Kexp⁡(wk(t−1)/T)∑iexp⁡(wi(t−1)/T),wk(t−1)=Lk(t−1)Lk(t−2).</p><p>这里的 wk 则通过计算两个相邻的 time step 的 loss ratio 作为 descending rate。因此 wk 越小，收敛速率就越大，任务就越简单，得到权重也就越小。</p><ul><li><strong>MTL as Multi-objective Optimisation</strong></li></ul><p>之前介绍的几个 task-weighting 方法都基于一些特定的 heuristic，很难保证在 MTL optimisation 取得 optimum. 在 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.04650">MTL as Multi-objective Optimisation</a> 里，作者将 MTL 问题看做是多目标优化问题，其目标为取得 Pareto optimum.</p><p>Pareto optimum 是指任何对其中一个任务的 performance 变好的情况，一定会对其他剩余所有任务的 performance 变差。作者利用了一个叫 multiple gradient descent algorithm (MGDA) 的方法来寻找这个 Pareto stationary point。大致方式是，在每次计算 task-specific gradients 后，其得到 common direction 来更新 shared parameter s。这个 common direction 如果存在，则整个 optimisation 并未收敛到 Pareto optimum。这样的收敛方法保证了 shared parameter 不会出现 conflicting gradients 让每一个任务的 loss 收敛更加平滑。</p><ul><li><strong>Other heuristic methods</strong></li></ul><p>对于 MTL loss function 的设计，还有其他不同的 heuristic 方法，或基于任务的难易性 (<a href="https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Michelle_Guo_Focus_on_the_ECCV_2018_paper.pdf">Dynamic Task Prioritization</a>), 或直接对于计算到的任务梯度进行映射，防止出现任务梯度之间的 conflicting gradients 情况 (<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2001.06782">Gradient Surgery</a>)。</p><p>但在各式各样的 MTL loss function design 里，很难出现其中一个方法在所有数据集里都 outperform 其他方法的情况。甚至，在部分数据集里，最简单的 equal task weighting 也表现得较为优异。一方面，task weighting 的有效性非常依赖于 MTL 网络本身的设计；此外 task weighting 的更新也依赖于数据集和 optimiser 本身。假设如果核心目标仅仅是取得最好的 MTL performance，那我建议应该花更多的时间去研究更好的网络而不是 task weighting。但不可否认的是，task weighting 的研究可以更好的帮助人们理解任务之间的相关性和复杂性，以此反过来帮助人们更好的设计模型本身。</p><hr /><h2 id="auxiliary-learning-not-all-tasks-are-created-equal-辅助学习"><a class="markdownIt-Anchor" href="#auxiliary-learning-not-all-tasks-are-created-equal-辅助学习"></a> Auxiliary Learning – Not all tasks are created equal [辅助学习]</h2><p>跟 MTL 高度相关的一个方向被称之为 Auxiliary learning (AL, 辅助学习)：他的训练过程与 MTL 完全一致。唯一的不同是，在 Auxiliary Learning 里，只有部分任务的 performance 是需要被考虑的 (primary task)，其他（辅助）任务 (auxiliary task) 的存在的意义是，帮助那部分需要被考虑的任务学习到更好的 representation。</p><ul><li><strong>Supervised Auxiliary Learning</strong></li></ul><p>Auxiliary Learning 存在的普遍性其实远超于我们的想象。比如，MTL 其实就是一种特殊形式的 AL，我们可以把其中任意一个 task 作为 primary task，其他剩余的 task 看作为 auxiliary task。在 MTL 里，我们默认所有 task 与 task 存在一种 mutual beneficial 的关系，因此所有 learning tasks 都 benefit 到这种相关性。</p><p>Auxiliary Learning 还应用在很多领域里，比如 <a href="https://link.zhihu.com/?target=http%3A//papers.neurips.cc/paper/7406-revisiting-multi-task-learning-with-rock-a-deep-residual-auxiliary-block-for-visual-detection.pdf">这篇文章</a> 发现在训练 depth 和 normal prediction 的同时，可以有效的帮助 object detection 的精确度。或者 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1803.00144.pdf">这篇文章</a> 发现在做 short sequence 的重建时，可以帮助 RNN 更有效的训练 very long sequence。</p><p>在 supervised auxiliary learning 的 setting 中，整个网络和任务的选择非常依赖于人类的先验知识，并不具备绝对的普遍性。</p><ul><li><strong>Meta Auxiliary Learning</strong></li></ul><p>考虑 supervised auxiliary learning 对于任务选择的局限性，我后续提出了一种基于 meta learning 的方法来自动生成 auxiliary task，我把这种方法称之为 Meta Auxiliary Learning (MAXL)。</p><p>在传统的 supervised auxiliary learning，我发现有这样如下两个规律：</p><ol><li>假设 primary 和 auxiliary task 是在同一个 domain，那么 primary task 的 performance 会提高当且仅当 auxiliary task 的 complexity 高于 primary task。</li><li>假设 primary 和 auxiliary task 是在同一个 domain，那么 primary task 的最终 performance 只依赖于 complexity 最高的 auxiliary task。</li></ol><p>这里对于 task 的 complexity 的定义比较 tricky 并不 general，目前我只考虑了最简单的图片分类的情况：细分类任务的 complexity 是高于粗分类任务。</p><p>比如在下图，我们看到猫，狗两类的分类的信息，直觉上一定低于细分类，约克夏，波斯猫之类更为细节的信息。而分类猫狗所需要的信息是分类细分类的子集，因此得出了规律 2.</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-3138d895136cb8fb3fc988c3edd7fa83_720w.webp" alt="img" /></p><p>再由于规律 2，我们只需要考虑最简单的两个任务训练的情况： primary task 和 auxiliary task 各为一个任务。因此在这里，我们”生成“一个好的 auxiliary task的问题，也就可转化为对任意输入图片，我们需要有一个好的网络去生成好的细分类标签。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-e4f37c12d38db8c1d74dca1f0baaf8c0_720w.webp" alt="img" /></p><p>Visualisation of MAXL</p><p>因此在 MAXL 框架里，我们有两个网络：一个网络是 multi-task network 类同于 hard parameter sharing 来做 multi-task training。另外一个网络是 label-generation network 来生成细分类标签，给 multi-task network 作为 auxiliary task 的 prediction label。</p><p>于是对于 Multi-task network fθ1(⋅) ，我们需要做以下更新：</p><p>argminθ1(L(fθ1pri(xtrain(i)),ytrain(i))+L(fθ1aux(xtrain(i)),gθ2(xtrain(i),ytrain(i),ψ))).</p><p>对于 label-generation network gθ2(⋅) , 我们需要通过 meta update 的方式做以下更新：</p><p>argminθ2L(fθ1+pri(xval(i)),yval(i))</p><p>其中，</p><p>θ1+=θ1−α∇θ1(L(fθ1pri(xval(i)),yval(i))+L(fθ1aux(xval(i)),gθ2(xval(i),yval(i),ψ)).</p><p>这里， θ1+ 代表了 meta update，来通过 maximise validation performance of primary task 来寻找最好的 auxiliary label。这里的 second derivative trick 类同于经典 meta learning 算法 MAML 的梯度更新。</p><p>在 label-generation network 里还存在一个 hyper-parameter ψ 代表人类定义的 dataset hierarchy。 假设我们在做简单的二分类，那么 ψ=[2,3] 则意味着将第一个类再细分成 2 类，第二个类再细分成 3 类。那么 label-generation network 就以这样的 hierarchy 通过 masked version of softmax 来生成相应的合适的 auxiliary label。于是 multi-task network 就在进行两个分类任务： primary task 是二分类，auxilairy task 是五分类。</p><p>通过 MAXL，我们发现他可以对这样的图片分类任务有着一定的效果提升。我们后续 visualize 生成的标签，发现在一些简单的数据集里有着人类可理解的 clustering 含义。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-3e30d57ee78b8c90a5cb7436c24ab5a7_720w.webp" alt="img" /></p><p>在上图中，上半部分是 CIFAR-100 的分类，下半部分是 MNIST 的分类。其中这三类 auxiliary class 中的图片是通过 label-generation network 生成在这个 class 里 5 个 confidence 最高的图片。</p><p>在较为复杂的 CIFAR-100 数据集中，我们很难理解 MAXL 的分类到底在干什么。而在 MNIST 中，我们可以发现 不同粗细的 数字 3，不同方向的 数字 9，有无中间的 horizontal bar 的数字 7 cluster 到了一起。这种有趣的现象开拓了一个新颖的方向，对自动化辅助任务生成的探索。</p><ul><li><strong>Self-supervised Auxiliary Learning: Learning to X by Y</strong></li></ul><p>Auxiliary learning 还可以应用在 self-supervised learning 中，假设对于 primary task 和 auxiliary task 都没有任何 human label。这里的 self-supervised training 跟跟 supervised auxiliary task 一样，是有效得利用了人类先验知识对于部分任务组合的特性。</p><p>较为常见的组合是：<a href="https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.pdf">depth 和 ego-motion</a> 的 prediction 可以在时间上具备 consistency。同样利用 time consistency 的还有 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1806.09594.pdf">colorisation 和 tracking</a>, <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1505.01596.pdf">feature learning 和 ego-motion</a>.</p><p>同样的这样的 consistency 也可以存在基于 RL 的robot learning 里，例如 <a href="https://link.zhihu.com/?target=https%3A//ieeexplore.ieee.org/stamp/stamp.jsp%3Farnumber%3D8593986%26casa_token%3DSYrRiphCo_4AAAAA%3A6LB6LeCVlb8xZhw06oKy7BR0NBlhPVN8XVyA1Xcqpeun5YqX6x7dagIBpVFzeNUQw12_-7K7%26tag%3D1">grasping 和 pushing</a>，或者 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1810.03043.pdf">robot manipulation</a>。</p><h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2><p>终于到总结了！近些年来 MTL 的研究出现了很多新颖且有价值的工作，但是对于任务自身的理解，和任务之间关系的理解还是有很大的不足和进步空间。在 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.08328">Taskonomy</a> 里，作者尝试了上千种（大量 CO2 排放）任务的组合来绘制出不同任务之间的关系图。但是真实 MTL 训练中，我相信这种关系图应该随着时间的变化而变化，且依赖网络本身。因此，如何更好得通过任务之间的关系去优化网络结构还是一个未解之谜，如何设计/生成辅助任务并通过 MTL 更好得帮助 primary task 也并未了解透彻。希望在后续的研究中能看到更多文章对于 MTL 的深入探索，实现 universal representation 的最终愿景。</p><p>写于五月七日，在疫情笼罩中的伦敦里摸鱼。</p><p>转载只需注明作者和来源即可，无需私信确认。</p><p>编辑于 2020-05-08 02:07</p><p><a href="https://zhuanlan.zhihu.com/p/138597214">Multi-task Learning and Beyond: 过去，现在与未来 - 知乎 (zhihu.com)</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch模型保存与加载</title>
    <link href="/2023/03/08/pytorch%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/"/>
    <url>/2023/03/08/pytorch%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch模型保存与加载"><a class="markdownIt-Anchor" href="#pytorch模型保存与加载"></a> pytorch模型保存与加载</h1><h2 id="第一种方式"><a class="markdownIt-Anchor" href="#第一种方式"></a> 第一种方式</h2><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230210141948373.png" alt="image-20230210141948373" /></p><blockquote><p>采用这种方式保存的模型是模型的结构和模型的参数。</p></blockquote><h3 id="在加载模型时"><a class="markdownIt-Anchor" href="#在加载模型时"></a> 在加载模型时：</h3><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230210142011454.png" alt="image-20230210142011454" /></p><p>此时模型的参数也被保存下来了，可以通过如下方式查看，即通过debug的模式，在的模型加载之后查看模型的参数：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230210142038612.png" alt="image-20230210142038612" /></p><h2 id="第二种方式"><a class="markdownIt-Anchor" href="#第二种方式"></a> 第二种方式</h2><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230210141746039.png" alt="image-20230210141746039" /></p><blockquote><p>这种方式保存的是模型的参数，其中模型后面的<code>vgg16.state_dict()</code> 表示将模型的参数保存为字典类型的数据。这是一种比较推荐的保存方式，因为，如果是自己定义的模型采用第一种的保存方式则在别的地方加载这个模型的时候就会找不到模型的结构。</p></blockquote><h3 id="第二种方式的模型加载"><a class="markdownIt-Anchor" href="#第二种方式的模型加载"></a> 第二种方式的模型加载</h3><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230210142451555.png" alt="image-20230210142451555" /></p><blockquote><p>这种方式保存的模型首先要取得模型的结构，然后通过<code>load_state_dict(torch.load('model.pth'))</code> 来将之前保存的模型的参数加载到模型当中。</p></blockquote><blockquote><p>通常来说应该首先采用第二种方式保存和加载模型。</p></blockquote><h2 id="自定义模型的保存和加载"><a class="markdownIt-Anchor" href="#自定义模型的保存和加载"></a> 自定义模型的保存和加载</h2><p>如果采用第一种方式保存：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230210142828356.png" alt="image-20230210142828356" /></p><p>在另一个文件中加载保存的这个模型时：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230210142947692.png" alt="image-20230210142947692" /></p><p>会出错，即找不到这个模型的结构。</p><p>此时需要将原先的网络模型的结构复制到或者通过<code>import</code> 引入到这个文件中：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230210143124732.png" alt="image-20230210143124732" /></p><p>这样才可以正常加载模型的参数，这样与最开始模型的定义与实现不一样的地方在于：不需要写<code>model=Tudui()</code> 这句话来创建模型的对象。</p><blockquote><p>所以一帮保存和加载模型都使用第二种方式。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像相似度算法及其适用场景分析</title>
    <link href="/2023/03/08/%E3%80%90%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E3%80%91%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%AE%97%E6%B3%95%E6%8E%A2%E7%A9%B6%E5%8F%8A%E5%85%B6%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90/"/>
    <url>/2023/03/08/%E3%80%90%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E3%80%91%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%AE%97%E6%B3%95%E6%8E%A2%E7%A9%B6%E5%8F%8A%E5%85%B6%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="图像相似度算法及其适用场景分析"><a class="markdownIt-Anchor" href="#图像相似度算法及其适用场景分析"></a> 图像相似度算法及其适用场景分析</h1><h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> <strong>前言</strong></h2><p>工作中，我们可能会碰到一些需求，需要比较不同图像的相似度，或者从大量图片中快速找到相似图片，这就需要借助相应的图像相似度算法来帮助我们实现。另外，机器学习和人工智能的大部分应用场景，都需要借助图像相似度算法，这也算是学习AI的重要一步。所以，在这里，我们一起探究主流的图像相似度算法，并尝试用OpenCV和Python来做简单实现。最后，根据不同相似度算法的优劣，总结出其对应的适用场景。</p><p>在比较两个图片的时候，我们总会有一些不同的需求，有时候我们希望能精确的比较两个图片是否完全相等，有时候，我们希望仅仅比较图像的相似度。</p><p>想要研究这个是因为最近我们碰到一个问题，我们想要检测两个图片是否相等，当这两个图片完全相同的时候，我们希望可以直接了当的返回一个True，当这两个图片有一丢丢的差距的时候，我们也希望可以返回True。但是如果严格用差值法对比图片，第二个需求就不满足，所以需要通过研究图像的相似性来解决目前的问题。</p><h3 id="图像相等的严格判别法"><a class="markdownIt-Anchor" href="#图像相等的严格判别法"></a> <strong>图像相等的严格判别法</strong></h3><p>比较两个图片是否相等，有很多方法，最直观的，也最容易想到的算法就是遍历两个图片的每一个像素，最终求差来得到两个图片是否相等。</p><p>使用OpenCV和Python语言实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compare_img_default</span>(<span class="hljs-params">img1, img2</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Strictly compare whether two pictures are equal</span><br><span class="hljs-string">        Attention: Even just a little tiny bit different (like 1px dot), will return false.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    :param img1: img1 in MAT format(img1 = cv2.imread(image1))</span><br><span class="hljs-string">    :param img2: img2 in MAT format(img2 = cv2.imread(image2))</span><br><span class="hljs-string">    :return: true for equal or false for not equal</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    difference = cv.subtract(img1, img2)<br>    result = <span class="hljs-keyword">not</span> np.<span class="hljs-built_in">any</span>(difference)<br><br>    <span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure><p>这种方式的优点是速度很快，比较精度特别高，但是如果两个图片稍微有一点不同，哪怕只是一个像素的差距，都会返回False。</p><p>因此，该算法的适用性并不广泛，除非你的需求就是要判断两张图片严格相等，但是多数情况下，我们需求都是比较两张图片是否相似。所以，接下来，我们来看看市面上主流的图像相似度的判别算法。</p><h3 id="图像相似的判别法"><a class="markdownIt-Anchor" href="#图像相似的判别法"></a> <strong>图像相似的判别法</strong></h3><p>图像相似也有很多判别法，我没有去尝试每一种方法，但是基本上所有的方法都了解了一遍，在这里总结一下主流的算法，同时也是巩固一下。</p><h3 id="直方图比较法"><a class="markdownIt-Anchor" href="#直方图比较法"></a> <strong>直方图比较法</strong></h3><p>直方图比较法是一个比较基础，也比较简单快捷的一种办法，目前我就是使用这个方法作为项目的主力相似性判别法。</p><p>直方图比较的原理是，将所要比较的两幅图片的直方图数据，然后再将直方图数据归一化之后方便比较，最终得到一个相似指数，通过设定相似指数的边界，我们可以得到是否是同一张图片。</p><p>这里可能涉及到一个直方图匹配的概念，但是我们其实用不到直方图匹配，<a href="https://zhuanlan.zhihu.com/p/94081111/%5Bhttps://baike.baidu.com/item/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%8C%B9%E9%85%8D%5D(https://baike.baidu.com/item/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%8C%B9%E9%85%8D)">直方图匹配</a>的概念是：</p><blockquote><p>直方图规定化(直方图匹配)是将变换过程加以控制，能够修正直方图的形状，或得到具有指定直方图的输出图像。有选择地增强某个灰度范围内的对比度或使图像灰度值满足某种特定的分布</p></blockquote><p>这个在图像处理中会涉及到，目前图像比较还没有涉及到，所以不做过多深入。</p><p>下面，用OpenCV和Python实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compare_img_hist</span>(<span class="hljs-params">img1, img2</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Compare the similarity of two pictures using histogram(直方图)</span><br><span class="hljs-string">        Attention: this is a comparision of similarity, using histogram to calculate</span><br><span class="hljs-string"></span><br><span class="hljs-string">        For example:</span><br><span class="hljs-string">         1. img1 and img2 are both 720P .PNG file,</span><br><span class="hljs-string">            and if compare with img1, img2 only add a black dot(about 9*9px),</span><br><span class="hljs-string">            the result will be 0.999999999953</span><br><span class="hljs-string"></span><br><span class="hljs-string">    :param img1: img1 in MAT format(img1 = cv2.imread(image1))</span><br><span class="hljs-string">    :param img2: img2 in MAT format(img2 = cv2.imread(image2))</span><br><span class="hljs-string">    :return: the similarity of two pictures</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Get the histogram data of image 1, then using normalize the picture for better compare</span><br>    img1_hist = cv.calcHist([img1], [<span class="hljs-number">1</span>], <span class="hljs-literal">None</span>, [<span class="hljs-number">256</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">256</span>])<br>    img1_hist = cv.normalize(img1_hist, img1_hist, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, cv.NORM_MINMAX, -<span class="hljs-number">1</span>)<br><br>    img2_hist = cv.calcHist([img2], [<span class="hljs-number">1</span>], <span class="hljs-literal">None</span>, [<span class="hljs-number">256</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">256</span>])<br>    img2_hist = cv.normalize(img2_hist, img2_hist, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, cv.NORM_MINMAX, -<span class="hljs-number">1</span>)<br><br>    similarity = cv.compareHist(img1_hist, img2_hist, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">return</span> similarity<br></code></pre></td></tr></table></figure><p>直方图算法运行速度很快，也是比较图片相似度算法中很受欢迎的算法，如果你的项目中，需要比较有着极高相似度的图片，而且你需要将这种具有极高相似度的图片归为同一个图片，那么就选用直方图算法吧！</p><h3 id="感知哈希算法"><a class="markdownIt-Anchor" href="#感知哈希算法"></a> <strong>感知哈希算法</strong></h3><p>感知哈希算法（pHash，全拼：Perceptual hash algorithm）是哈希算法的一种，哈希算法还包括平均值哈希算法（aHash），差异值哈希算法（dHash），经过对比之后，最终决定采用pHash算法来作为辅助相似性判别法。</p><p>pHash简单来说，是通过感知哈希算法对每张图片生成一个“指纹”字符串，然后通过比较“指纹”字符串的距离（通常采用汉明距离，Hamming distance，两个等长字符串之间的汉明距离，是两个字符串对应位置的不同字符的个数），这个距离越小，代表两个图片越相似，一般的，我们有下面规则：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">Hamming distance = 0  -&gt; particular like<br>Hamming distance &lt; 5  -&gt; very like<br>hamming distance &gt; 10 -&gt; different picture<br></code></pre></td></tr></table></figure><p>关于aHash, pHash, dHash的区别，我总结如下：</p><ul><li>aHash：平均值哈希。转灰度压缩之后计算均值，最终通过像素比较得出哈希值，速度很快，但敏感度很高，稍有变化就会极大影响判定结果，精准度较差。因此比较适用于缩略图比较，最常用的就是以图搜图。</li><li>pHash：感知哈希。在均值哈希基础上加入DCT（离散余弦变化，下面也会涉及，这里不展开），两次DCT就可以很好的将图像按照频度分开，取左上角高能低频信息做均值哈希，因此，精确度很高，但是速度方面较差一些。<br />Opencv实测速度比平均哈希慢大概200ms～700ms左右（我自己的开发机，不同机器，不同版本可能结果也会不同）。相比较aHash，pHash更加适合用于缩略图比较，也非常适合比较两个近似图片是否相等。<br />比如你所要比较的两张图片就只有一个button有变化，其余的都没有变化，采用严格的aHash很有可能就得到错误的结果，这种情况就可以放心大胆的采用pHash算法进行比较。实际上，这个场景就是我们项目中实际碰到的，最终也是通过pHash来解决的。</li><li>dHash：差异值哈希。转灰度压缩之后，比较相邻像素之间差异。<br />假设有10×10的图像，每行10个像素，就会产生9个差异值，一共10行，就一共有9×10=90个差异值。最终生成哈希值即指纹。<br />速度上来说，介于aHash和pHash之间，精准度同样也介于aHash和pHash之间。所以前两种哈希的适用场景，dHash也完全适用，如果你想要一个精准度较高而且速度比较快的算法，那就选择dHash吧！</li></ul><p>我们首先来简单的了解一下均值哈希算法，其原理如下（<a href="https://link.zhihu.com/?target=http%3A//www.hackerfactor.com/blog/index.php%3F/archives/432-Looks-Like-It.html">来源</a>）：</p><blockquote><p><strong>1. Reduce size</strong>. The fastest way to remove high frequencies and detail is to shrink the image. In this case, shrink it to 8x8 so that there are 64 total pixels. Don’t bother keeping the aspect ratio, just crush it down to fit an 8x8 square. This way, the hash will match any variation of the image, regardless of scale or aspect ratio.<br /><strong>2. Reduce color</strong>. The tiny 8x8 picture is converted to a grayscale. This changes the hash from 64 pixels (64 red, 64 green, and 64 blue) to 64 total colors.<br /><strong>3. Average the colors</strong>. Compute the mean value of the 64 colors.<br /><strong>4. Compute the bits</strong>. This is the fun part. Each bit is simply set based on whether the color value is above or below the mean.<br /><strong>5. Construct the hash</strong>. Set the 64 bits into a 64-bit integer. The order does not matter, just as long as you are consistent. (I set the bits from left to right, top to bottom using big-endian.)<br />The resulting hash won’t change if the image is scaled or the aspect ratio changes. Increasing or decreasing the brightness or contrast, or even altering the colors won’t dramatically change the hash value. And best of all: this is FAST!</p></blockquote><p>pHash在均值哈希算法的基础上，加入了离散余弦变换（DCT，Discrete cosine transform），数学不精通的同学可能这里就有点着急了，简单解释一下离散余弦变化，其作用是为了减少图像的冗余和相关性，是一种图像压缩算法，将图像从像素域变换到频率域。详细请<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E7%A6%BB%E6%95%A3%E4%BD%99%E5%BC%A6%E5%8F%98%E6%8D%A2">前往</a></p><p>我们用步骤来描述如何将DCT加入到pHash中：</p><ol><li>缩小尺寸：pHash以小图片开始，但图片大于8×8，32×32是最好的（我这里选用的是32×32的，计算量完全OK）。这样做的目的是简化了DCT的计算，而不是减小频率。</li><li>简化色彩（灰度化）：将图片转化成灰度图像，一个只由0～255整数来表示的灰度图，由此进一步简化计算量。</li><li>计算DCT：计算图片的DCT变换，得到32×32的DCT系数矩阵。</li><li>缩小DCT：虽然DCT的结果是32×32大小的矩阵，但我们只要保留左上角的8×8的矩阵，这部分呈现了图片中的最低频率。</li><li>计算平均值：如同均值哈希一样，计算DCT的均值。</li><li>计算hash值：这是最主要的一步，根据8×8的DCT矩阵，设置0或1的64位的hash值，大于等于DCT均值的设为”1”，小于DCT均值的设为“0”。组合在一起，就构成了一个64位的整数，这就是这张图片的指纹。</li></ol><p>基于以上步骤，我们实现了一个简单的pHash算法，同样，使用OpenCV和Python来实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compare_img_p_hash</span>(<span class="hljs-params">img1, img2</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Get the similarity of two pictures via pHash</span><br><span class="hljs-string">        Generally, when:</span><br><span class="hljs-string">            ham_dist == 0 -&gt; particularly like</span><br><span class="hljs-string">            ham_dist &lt; 5  -&gt; very like</span><br><span class="hljs-string">            ham_dist &gt; 10 -&gt; different image</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Attention: this is not accurate compare_img_hist() method, so use hist() method to auxiliary comparision.</span><br><span class="hljs-string">            This method is always used for graphical search applications, such as Google Image(Use photo to search photo)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    :param img1:</span><br><span class="hljs-string">    :param img2:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    hash_img1 = get_img_p_hash(img1)<br>    hash_img2 = get_img_p_hash(img2)<br><br>    <span class="hljs-keyword">return</span> ham_dist(hash_img1, hash_img2)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_img_p_hash</span>(<span class="hljs-params">img</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Get the pHash value of the image, pHash : Perceptual hash algorithm(感知哈希算法)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    :param img: img in MAT format(img = cv2.imread(image))</span><br><span class="hljs-string">    :return: pHash value</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    hash_len = <span class="hljs-number">32</span><br><br>    <span class="hljs-comment"># GET Gray image</span><br>    gray_img = cv.cvtColor(img, cv.COLOR_RGB2GRAY)<br><br>    <span class="hljs-comment"># Resize image, use the different way to get the best result</span><br>    resize_gray_img = cv.resize(gray_img, (hash_len, hash_len), cv.INTER_AREA)<br>    <span class="hljs-comment"># resize_gray_img = cv.resize(gray_img, (hash_len, hash_len), cv.INTER_LANCZOS4)</span><br>    <span class="hljs-comment"># resize_gray_img = cv.resize(gray_img, (hash_len, hash_len), cv.INTER_LINEAR)</span><br>    <span class="hljs-comment"># resize_gray_img = cv.resize(gray_img, (hash_len, hash_len), cv.INTER_NEAREST)</span><br>    <span class="hljs-comment"># resize_gray_img = cv.resize(gray_img, (hash_len, hash_len), cv.INTER_CUBIC)</span><br><br>    <span class="hljs-comment"># Change the int of image to float, for better DCT</span><br>    h, w = resize_gray_img.shape[:<span class="hljs-number">2</span>]<br>    vis0 = np.zeros((h, w), np.float32)<br>    vis0[:h, :w] = resize_gray_img<br><br>    <span class="hljs-comment"># DCT: Discrete cosine transform(离散余弦变换)</span><br>    vis1 = cv.dct(cv.dct(vis0))<br>    vis1.resize(hash_len, hash_len)<br>    img_list = vis1.flatten()<br><br>    <span class="hljs-comment"># Calculate the avg value</span><br>    avg = <span class="hljs-built_in">sum</span>(img_list) * <span class="hljs-number">1.</span> / <span class="hljs-built_in">len</span>(img_list)<br>    avg_list = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> img_list:<br>        <span class="hljs-keyword">if</span> i &lt; avg:<br>            tmp = <span class="hljs-string">&#x27;0&#x27;</span><br>        <span class="hljs-keyword">else</span>:<br>            tmp = <span class="hljs-string">&#x27;1&#x27;</span><br>        avg_list.append(tmp)<br><br>    <span class="hljs-comment"># Calculate the hash value</span><br>    p_hash_str = <span class="hljs-string">&#x27;&#x27;</span><br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, hash_len * hash_len, <span class="hljs-number">4</span>):<br>        p_hash_str += <span class="hljs-string">&#x27;%x&#x27;</span> % <span class="hljs-built_in">int</span>(<span class="hljs-string">&#x27;&#x27;</span>.join(avg_list[x:x + <span class="hljs-number">4</span>]), <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> p_hash_str<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ham_dist</span>(<span class="hljs-params">x, y</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Get the hamming distance of two values.</span><br><span class="hljs-string">        hamming distance(汉明距)</span><br><span class="hljs-string">    :param x:</span><br><span class="hljs-string">    :param y:</span><br><span class="hljs-string">    :return: the hamming distance</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(x) == <span class="hljs-built_in">len</span>(y)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>([ch1 != ch2 <span class="hljs-keyword">for</span> ch1, ch2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(x, y)])<br></code></pre></td></tr></table></figure><p>自认为代码中注释已经非常友好了，所以对于算法步骤不做过多说明了，开箱可用！</p><h3 id="其他相似性比较算法"><a class="markdownIt-Anchor" href="#其他相似性比较算法"></a> <strong>其他相似性比较算法</strong></h3><p>除了上述直方图和哈希算法之外，还有其他的一些算法，我列举如下：</p><ol><li>内容特征法<br />内容特征法和直方图比较法比较相似，只不过是从比较图像内容入手的，通常的做法是压缩灰度化之后来比较前景色和背景色的反差。<br />通过设定一个值，来区分前景色和背景色，我们前面简单说过，灰度化就是用0～255来表示所有的像素，我们设定一个值，比如说130，大于130的，我们称之为背景色，反之，称之为前景色。<br />最终，我们通过一个公式（被称为大津法，Otsu’s method，<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Otsu%27s_method">维基百科</a>）：<br />我们记：<br />前景色占比 F=前景色像素数/总体像素数<br />背景色占比 B=背景色像素数/总体像素数<br />前景色平均值和方差 F1，F2<br />背景色平均值和方差 B1，B2<br />​<br />则：<br />类内差异 = F(F2^2) + B(B2^2)<br />类间差异 = FB(F1-B1)^2<br />使得前景色和背景色的“类内差异最小”，得到一个阈值，将阈值和每一个像素比较之后，就得到了该图片的[0-1]特征矩阵，通过比较两个不同图片的特征矩阵即可得到图片之间的相似度。<br />该算法也除了用于图片相似度之外，也常用于找不同，找不同的精确度很高，速度相较于直方图，差距不明显，如果你有找不同的需求，可以使用这个算法来做，但是如果你有模糊匹配的需求，该算法就非常不合适了。<br />该算法也可用于媒体处理，如视频跟踪，运动检测等。</li><li>关键点匹配<br />或者叫特征点匹配，使用特征描述子来做角点检测，特征描述子通常是一个向量，两个向量之间的距离就可以用来比较其相似度。</li><li>SSIM（structural similarity，结构相似性）<br />这里摘抄一段维基百科关于PSNR的定义：<br /><strong>The</strong> structural similarity** (<strong>SSIM</strong>) index is a method for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos. The basic model was developed in the Laboratory for Image and Video Engineering(LIVE) at The University of Texas at Austin and further developed jointly with the Laboratory for Computational Vision (LCV) at New York University. Further variants of the model have been developed in the Image and Visual Computing Laboratory at University of Waterloo and have been commercially marketed.<br />SSIM作为一种全参考的图像相似度算法，从亮度、对比度和结构三方面来比较图像相似度，大名鼎鼎，使用广泛。<br />初期我也将其运用到项目中，但是实际运行效果发现，耗时有点长，而且结果还没有直方图来的简单直接，给人一种杀鸡焉用牛刀的感觉，所以最终还是放弃了。</li></ol><h3 id="结语"><a class="markdownIt-Anchor" href="#结语"></a> <strong>结语</strong></h3><p>文中列出的是主流的最常使用的一些算法，还有一些其他算法，没有列出，是因为在了解过程中，根据需求最终决定不去尝试，不然试错成本太高，有一些明显不合适的算法就简单了解就可以了。</p><p>最后，不是最好的就是最合适的，这里建议以项目驱动，满足项目需求即可。</p><ul><li></li></ul>]]></content>
    
    
    <categories>
      
      <category>图像处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像相似度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch余弦退火学习率CosineAnnealingLR的使用</title>
    <link href="/2023/03/08/%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    <url>/2023/03/08/%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB%E5%AD%A6%E4%B9%A0%E7%8E%87/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch余弦退火学习率cosineannealinglr的使用"><a class="markdownIt-Anchor" href="#pytorch余弦退火学习率cosineannealinglr的使用"></a> pytorch余弦退火学习率CosineAnnealingLR的使用</h1><h2 id="一-背景"><a class="markdownIt-Anchor" href="#一-背景"></a> 一、背景</h2><p>再次使用CosineAnnealingLR的时候出现了一点疑惑，这里记录一下，其使用方法和参数含义<br /><strong>后面的代码基于 pytorch 版本 1.1</strong>, 不同版本可能代码略有差距，但是含义是差不多的</p><h2 id="二-余弦退火的目的和用法"><a class="markdownIt-Anchor" href="#二-余弦退火的目的和用法"></a> 二、余弦退火的目的和用法</h2><h2 id="21-为啥用cosineannealinglr策略"><a class="markdownIt-Anchor" href="#21-为啥用cosineannealinglr策略"></a> 2.1 为啥用cosineAnnealingLR策略</h2><p>原因：因为懒… 这样就不用像使用其他类似于StepLR策略 进行调参了，而且总会取得不错的结果。</p><p>余弦函数如下（两个）<br /><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/2be0d5c5a82e4c00984963500ec52501.png" alt="在这里插入图片描述" /><br />考虑cosine函数的四分之一个周期，如下图所示<br /><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/45cac597c60a4d5f94cc9fb0ac0dc31a.png" alt="在这里插入图片描述" /></p><p>我们希望<code>学习率能像四分之一个cosine的周期</code>一样下降：所以有了<code>cosineAnnealingLR</code>学习率的策略。如果想每个batch 更新学习率，则</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, <span class="hljs-attribute">eta_min</span>=0, <span class="hljs-attribute">last_epoch</span>=- 1, <span class="hljs-attribute">verbose</span>=<span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p>``<br />这里面主要就介绍一下参数<code>T_max</code> ，这个参数指的是cosine 函数 经过多少次更新完成二分之一个周期。</p><h2 id="22-如果-希望-learning-rate-每个epoch更新一次"><a class="markdownIt-Anchor" href="#22-如果-希望-learning-rate-每个epoch更新一次"></a> 2.2 如果 希望 learning rate 每个epoch更新一次</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>net = models.resnet18(pretrained=<span class="hljs-literal">False</span>)<br><br><br>max_epoch=<span class="hljs-number">50</span> <span class="hljs-comment"># 一共50 epoch</span><br>iters=<span class="hljs-number">200</span>    <span class="hljs-comment"># 每个epoch 有 200 个 bach</span><br><br>optimizer = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>)<br>scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer,<br>                                                        T_max =  max_epoch) <span class="hljs-comment">#  * iters </span><br><br><br>lr = []                                       <br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_epoch):<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iters):<br>        optimizer.step()<br><br>      <br>        lr.append(scheduler.get_lr()[<span class="hljs-number">0</span>])<br>    scheduler.step() <span class="hljs-comment"># 注意 每个epoch 结束， 更新learning rate</span><br><br>plt.plot(np.arange(<span class="hljs-built_in">len</span>(lr)), lr)<br>plt.savefig(<span class="hljs-string">&#x27;aa.jpg&#x27;</span>)<br></code></pre></td></tr></table></figure><p>每个epoch内，learning rate 是一样的<br /><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/868b930777744202a26a1b3b4b8466e3.jpeg" alt="在这里插入图片描述" /></p><h2 id="23-每个batch-迭代都改变学习率"><a class="markdownIt-Anchor" href="#23-每个batch-迭代都改变学习率"></a> 2.3 每个batch 迭代都改变学习率</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-attribute">max_epoch</span>=50 # 一共50 epoch<br><span class="hljs-attribute">iters</span>=200    # 每个epoch 有 200 个 bach<br><br>optimizer = torch.optim.SGD(net.parameters(), <span class="hljs-attribute">lr</span>=0.01, <span class="hljs-attribute">momentum</span>=0.9)<span class="hljs-built_in"></span><br><span class="hljs-built_in">scheduler </span>=  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer,<br>                                                        T_max =  max_epoch * iters ) #  调整了四分之一周期的长度<br><br>lr = []                                       <br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(max_epoch):<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> range(iters):<br>        optimizer.<span class="hljs-keyword">step</span>()<br><br>      <br>        lr.append(scheduler.get_lr()[0])<br>    scheduler.<span class="hljs-keyword">step</span>() # 注意 每个batch 结束， 更新learning rate<br></code></pre></td></tr></table></figure><p>每个batch都改变学习率<br /><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/cea268fdee884ed09a2dc767f78f4f8c.jpeg" alt="在这里插入图片描述" /></p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自定义损失函数</title>
    <link href="/2023/03/08/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <url>/2023/03/08/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="自定义损失函数"><a class="markdownIt-Anchor" href="#自定义损失函数"></a> 自定义损失函数</h1><p>PyTorch在torch.nn模块为我们提供了许多常用的损失函数，比如：MSELoss，L1Loss，BCELoss… 但是随着深度学习的发展，出现了越来越多的非官方提供的Loss，比如DiceLoss，HuberLoss，SobolevLoss… 这些Loss Function专门针对一些非通用的模型，PyTorch不能将他们全部添加到库中去，因此这些损失函数的实现则需要我们通过自定义损失函数来实现。另外，在一些算法实现中，研究者往往会提出全新的损失函数来提升模型的表现，这时我们既无法使用PyTorch自带的损失函数，也没有相关的博客供参考，此时自己实现损失函数就显得更为重要了。</p><p>经过本节的学习，你将收获：</p><ul><li>掌握如何自定义损失函数</li></ul><h2 id="611-以函数方式定义"><a class="markdownIt-Anchor" href="#611-以函数方式定义"></a> 6.1.1 以函数方式定义</h2><p>事实上，损失函数仅仅是一个函数而已，因此我们可以通过直接以函数定义的方式定义一个自己的函数，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">my_loss</span>(<span class="hljs-params">output, target</span>):<br>    loss = torch.mean((output - target)**<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><h2 id="612-以类方式定义"><a class="markdownIt-Anchor" href="#612-以类方式定义"></a> 6.1.2 以类方式定义</h2><p>虽然以函数定义的方式很简单，但是以类方式定义更加常用，在以类方式定义损失函数时，我们如果看每一个损失函数的继承关系我们就可以发现<code>Loss</code>函数部分继承自<code>_loss</code>, 部分继承自<code>_WeightedLoss</code>, 而<code>_WeightedLoss</code>继承自<code>_loss</code>，<code>_loss</code>继承自 <strong>nn.Module</strong>。我们可以将其当作神经网络的一层来对待，同样地，我们的损失函数类就需要继承自<strong>nn.Module</strong>类，在下面的例子中我们以DiceLoss为例向大家讲述。</p><p>Dice Loss是一种在分割领域常见的损失函数，定义如下：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230709160015651.png" alt="image-20230709160015651" /></p><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiceLoss</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,weight=<span class="hljs-literal">None</span>,size_average=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(DiceLoss,self).__init__()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,inputs,targets,smooth=<span class="hljs-number">1</span></span>):<br>        inputs = F.sigmoid(inputs)       <br>        inputs = inputs.view(-<span class="hljs-number">1</span>)<br>        targets = targets.view(-<span class="hljs-number">1</span>)<br>        intersection = (inputs * targets).<span class="hljs-built_in">sum</span>()                   <br>        dice = (<span class="hljs-number">2.</span>*intersection + smooth)/(inputs.<span class="hljs-built_in">sum</span>() + targets.<span class="hljs-built_in">sum</span>() + smooth)  <br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> - dice<br><br><span class="hljs-comment"># 使用方法    </span><br>criterion = DiceLoss()<br>loss = criterion(<span class="hljs-built_in">input</span>,targets)<br></code></pre></td></tr></table></figure><p>除此之外，常见的损失函数还有BCE-Dice Loss，Jaccard/Intersection over Union (IoU) Loss，Focal Loss…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiceBCELoss</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(DiceBCELoss, self).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, targets, smooth=<span class="hljs-number">1</span></span>):<br>        inputs = F.sigmoid(inputs)       <br>        inputs = inputs.view(-<span class="hljs-number">1</span>)<br>        targets = targets.view(-<span class="hljs-number">1</span>)<br>        intersection = (inputs * targets).<span class="hljs-built_in">sum</span>()                     <br>        dice_loss = <span class="hljs-number">1</span> - (<span class="hljs-number">2.</span>*intersection + smooth)/(inputs.<span class="hljs-built_in">sum</span>() + targets.<span class="hljs-built_in">sum</span>() + smooth)  <br>        BCE = F.binary_cross_entropy(inputs, targets, reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br>        Dice_BCE = BCE + dice_loss<br>        <br>        <span class="hljs-keyword">return</span> Dice_BCE<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">IoULoss</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(IoULoss, self).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, targets, smooth=<span class="hljs-number">1</span></span>):<br>        inputs = F.sigmoid(inputs)       <br>        inputs = inputs.view(-<span class="hljs-number">1</span>)<br>        targets = targets.view(-<span class="hljs-number">1</span>)<br>        intersection = (inputs * targets).<span class="hljs-built_in">sum</span>()<br>        total = (inputs + targets).<span class="hljs-built_in">sum</span>()<br>        union = total - intersection <br>        <br>        IoU = (intersection + smooth)/(union + smooth)<br>                <br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> - IoU<br>ALPHA = <span class="hljs-number">0.8</span><br>GAMMA = <span class="hljs-number">2</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FocalLoss</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(FocalLoss, self).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=<span class="hljs-number">1</span></span>):<br>        inputs = F.sigmoid(inputs)       <br>        inputs = inputs.view(-<span class="hljs-number">1</span>)<br>        targets = targets.view(-<span class="hljs-number">1</span>)<br>        BCE = F.binary_cross_entropy(inputs, targets, reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br>        BCE_EXP = torch.exp(-BCE)<br>        focal_loss = alpha * (<span class="hljs-number">1</span>-BCE_EXP)**gamma * BCE<br>                       <br>        <span class="hljs-keyword">return</span> focal_loss<br><span class="hljs-comment"># 更多的可以参考链接1</span><br></code></pre></td></tr></table></figure><p><strong>注：</strong></p><p>在自定义损失函数时，涉及到数学运算时，我们最好全程使用PyTorch提供的张量计算接口，这样就不需要我们实现自动求导功能并且我们可以直接调用cuda，使用numpy或者scipy的数学运算时，操作会有些麻烦，大家可以自己下去进行探索。关于PyTorch使用Class定义损失函数的原因，可以参考PyTorch的讨论区（链接6）</p><h2 id="本节参考"><a class="markdownIt-Anchor" href="#本节参考"></a> 本节参考</h2><p>【1】<a href="https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook">https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/notebook</a><br />【2】<a href="https://www.zhihu.com/question/66988664/answer/247952270">https://www.zhihu.com/question/66988664/answer/247952270</a><br />【3】<a href="https://blog.csdn.net/dss_dssssd/article/details/84103834">https://blog.csdn.net/dss_dssssd/article/details/84103834</a><br />【4】<a href="https://zj-image-processing.readthedocs.io/zh_CN/latest/pytorch/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">https://zj-image-processing.readthedocs.io/zh_CN/latest/pytorch/自定义损失函数/</a><br />【5】<a href="https://blog.csdn.net/qq_27825451/article/details/95165265">https://blog.csdn.net/qq_27825451/article/details/95165265</a><br />【6】<a href="https://discuss.pytorch.org/t/should-i-define-my-custom-loss-function-as-a-class/89468">https://discuss.pytorch.org/t/should-i-define-my-custom-loss-function-as-a-class/89468</a></p><p>原文链接：[6.1 自定义损失函数 — 深入浅出PyTorch (<a href="http://datawhalechina.github.io">datawhalechina.github.io</a>)](<a href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.1">https://datawhalechina.github.io/thorough-pytorch/第六章/6.1</a> 自定义损失函数.html)</p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>超参数搜索</title>
    <link href="/2023/03/08/%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"/>
    <url>/2023/03/08/%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="超参数搜索"><a class="markdownIt-Anchor" href="#超参数搜索"></a> 超参数搜索</h1><p>最近遇到了搜索超参数规模过大的问题，需要一些自动调参的方法，下面是一些调研。</p><h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2><p>这个文档的主要参考资料来自网络，我只是想用这个工具，并没有深入的去研究细节。但是超参数搜索在深度神经网络里面很重要，很多云服务厂商，包括华为云的ModelArts等等，都提供了相应的服务，因此应该还是比较完整的一个调研。</p><p>超参数是开始训练之前，用预先确定的值来手动设置的所有训练变量。这里的超参数要区别于训练过程中产生的参数，这些确实也是参数，但是并不需要我们手动设置。下面是一个深度神经网络的例子，图中，学习率、dropout比例以及batch大小就是超参数，但是每层神经网络的参数，虽然也很重要，但是我们并不很关心，也不需要我们手动去设置，因此就不是超参数。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-2b143c9c5c943a2e1170f39001b23758_1440w.webp" alt="img" /></p><p>训练过神经网络的都知道，调参是一门玄学，是需要不断设计优化并且有一定的理论支持的。之所以会这样，我认为主要是因为我们梯度下降法，虽然最后或许能收敛到某个极小点，但是是否能到达，会不会走过了，能不能到某一个固定的极小值点，其实都是不确定的。这些都依赖于参数的选择，目前还没有很好的解决方法。例如下图就给了两个不同的路径，不断优化之后函数确实收敛了，但是得到的结果是截然不同的。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-4fa5468483c69dd8e83a597594c7b51b_1440w.webp" alt="img" /></p><p>而超参数搜索就是一个找到最优超参数的过程，这是很多机器学习从业者的追求，但是也面临着很多困难。目前有一些搜索方法，大致可以总结为以下几个：</p><ul><li>Babysitting</li><li>Grid Search</li><li>Random Search</li><li>Optimization</li></ul><h3 id="babysitting"><a class="markdownIt-Anchor" href="#babysitting"></a> Babysitting</h3><p>Babysitting是最常用的方法，纯手工操作。简单点讲，这个方法就是玄学调参。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-da3563f4fe37ef7f9b9e46c8cefdd345_1440w.webp" alt="img" /></p><p>具体来讲，babysitting的方法就是手动的设置一个参数，然后根据一些经验来判断这些参数的好坏，然后在时间还足够的情况下不断地去优化改进。上面是一个经典的机器学习的流程图，如果我们也是基于机器学习的方法去调参的话，我们每次就都需要将这一流程重新都一遍，并根据经验去判断。</p><p>一般来说这一方法还是挺好用的，因为在不同的任务上总有一定的经验知识。特别是学生在初学的时候，这个过程也很有利于学生对算法的理解(和摸鱼)，但是显然这不是很好的方法，也很不自动化。</p><h3 id="grid-search"><a class="markdownIt-Anchor" href="#grid-search"></a> Grid Search</h3><p>好，现在我们要开始自动搜索了。之前babysitting方法，我们其实是心里面有一个可行的参数范围，然后根据一些经验不断地去尝试更好的方法。那么一个最直接的自动搜索方法就是，我直接把我心里面的可行的参数范围都搜一遍就可以了。这就是Grid Search。这其实就是一个遍历的思路，之所以叫Grid Search，可以看下图。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-013cb6fe769f4fc2b027fc527001e46a_1440w.webp" alt="img" /></p><p>例如我们现在要对学习率和dropout的比例进行遍历，那应该是两层循环。这意味着我们需要将两个参数可能出现的值的各种组合都试一遍，那不就构成了一个Grid吗。</p><p>这个方法有多个好处。首先，我们将所有可能的情况都试了一遍，那么其实我们可以说，我们找到了最优解了；其次，在不考虑计算资源限制的情况下，这些参数调试的计算其实是可以并行进行的，这意味着我们可以用空间换时间。如果做一次实验需要的时间是1ms，那么这并不重要；但是现在的大规模神经网络训练动辄好几天甚至一周，那么这就很重要了。</p><h3 id="random-search"><a class="markdownIt-Anchor" href="#random-search"></a> Random Search</h3><p>计算机科学里面有一些很有趣的现象，有的时候我们精心设计的方案还没有我们随机搜一个效果好，例如这个随机搜索。</p><p>我们考虑下面左图的网格搜索，看上去每种情况都已经考虑到了，但是其实并不然。网格搜索其实也只是搜索了这么一个子空间的一小部分，准确的说是那几个点。我们认为当网格足够密集的时候，可以认为遍历了整个子空间，但是网络越密集，搜索代价越大，在同样的搜索成本下，我们只能搜有限的几个点，这个时候效率就显得比较重要了。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-cec53b7edc8e5a5d6c859bbd9859b508_1440w.webp" alt="img" /></p><p>图中，同样是搜9个点，网格搜索的情况下，重要的参数其实只取了三个不同的值，这其实效率并不高，因为有的时候网络的效果只和这个参数有关，所以只相当于做了三次实验。而如果随机搜索的话，那么可以看出，我随机得到的点，重要的参数取到了9个不同的值，显然效果更高。</p><h3 id="optimization"><a class="markdownIt-Anchor" href="#optimization"></a> Optimization</h3><p>但是这些搜索方法其实还是存在组合爆炸的问题，规模一大，就很难cover住所有的结果了，这个时候，优化的方法就显得比较重要了。目前公认的效果比较好的优化方法是贝叶斯优化，此外，基于进化算法的优化也较为常见。</p><p>优化的方法的思路是这样的。假设我们现在有一个函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>:</mo><mi>x</mi><mo>→</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">f:x\to R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span></span></span></span>，我们需要在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>⊆</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">x \subseteq X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7719400000000001em;vertical-align:-0.13597em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⊆</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>内找到 $$ x^* = \arg\min_{x \in X}f(x) $$ 其中，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>为超参数，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>为问题的定义域。显然，当凸的时候，我们可以使用凸优化的方法来解决这一问题，但是，现在我们面临的问题往往是expensive black-box function，这就遇到了困难。</p><h3 id="贝叶斯优化"><a class="markdownIt-Anchor" href="#贝叶斯优化"></a> 贝叶斯优化</h3><p><strong>Sequential model-based optimization (SMBO)</strong> 是贝叶斯优化的最简形式，其算法思路如下：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-e91606bbe0872448a276b5879574ef3b_1440w.webp" alt="img" /></p><p>简单来讲，就是采用一个模型在现有的数据点上不断地去逼近，每次选到的最好结果都加入现有的数据点中再进行拟合。贝叶斯优化往往采用高斯模型去拟合，当然也有一些变种，在此不进行展开。</p><p>大致示意图如下所示，本来是用一个高斯模型去拟合，每加一个点，不确定性就增大一些。</p><p><img src="https://pic4.zhimg.com/80/v2-d47cbbdedb411732f09628106020ec07_1440w.webp" alt="img" /></p><p>假设我可以做无穷多次实验，那么我这个拟合就可以很准确。如果我只能做几次实验，那么我的结果也不会很准确。这里就存在一个取舍问题了。</p><h3 id="进化算法优化"><a class="markdownIt-Anchor" href="#进化算法优化"></a> 进化算法优化</h3><p>进化算法是一类仿生算法，借鉴了生物进化过程中的一些特性，是一种比较鲁棒的优化算法，包含很多类别，例如遗传算法、粒子群算法等等，大致思路都差不多。</p><p>例如遗传算法，就是模拟了遗传这一过程。下图展示了这一模拟。首先我们会初始化一些基因，然后会选择表现比较好的基因作为父方和母方，进行遗传操作。基因的遗传操作包括交叉与变异，交叉保证了基因的遗传，变异保证了新的基因的出现，因此都是有必要的。然后我们再对产生的新的基因进行评估，重复这一过程，直到收敛。</p><p><img src="https://pic2.zhimg.com/80/v2-268b94eb58cd53572ac70b2263f13269_1440w.webp" alt="img" /></p><p>经历这一过程，我们便可以较为快速地对参数进行优化。但是显然，这一优化的结果并不保证是最优的。</p><h2 id="后记"><a class="markdownIt-Anchor" href="#后记"></a> 后记</h2><p>这篇文章对现有的超参数优化的方法进行了一些简单的调研。由于NAS专注于神经网络架构的搜索，本文旨在解决超参数搜索的问题，不局限于神经网络，因此并没有提到。现有的超参数优化方法其实都还比较初级，还没有一个很好的方法去搜索超参数，各个方法都有自己的优势与劣势。即使是最简单的babysitting方法，也有可以带入专家经验的优势在里面。因此在实际使用过程中，我们还是需要按照自己的需要灵活。</p><h2 id="参考文献"><a class="markdownIt-Anchor" href="#参考文献"></a> 参考文献</h2><p>[1]《深度学习超参数搜索实用指南》, 知乎专栏. <a href="https://zhuanlan.zhihu.com/p/46278815">https://zhuanlan.zhihu.com/p/46278815</a> (见于 11月 21, 2020).</p><p>[2]《贝叶斯优化(Bayesian Optimization)深入理解》, 知乎专栏. <a href="https://zhuanlan.zhihu.com/p/53826787">https://zhuanlan.zhihu.com/p/53826787</a> (见于 11月 21, 2020).</p><p>[3]《梯度下降（Gradient Descent）小结 - 刘建平Pinard - 博客园》. <a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/5970503.html">https://www.cnblogs.com/pinard/p/5970503.html</a> (见于 11月 21, 2020).</p><p>[4]《遗传算法和超参数优化》, 知乎专栏. <a href="https://zhuanlan.zhihu.com/p/123319468">https://zhuanlan.zhihu.com/p/123319468</a> (见于 11月 21, 2020).</p><p>[5]《遗传算法详解（GA）boat_lee的博客-CSDN博客_遗传算法》. <a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/u010451580/article/details/51178225">https://blog.csdn.net/u010451580/article/details/51178225</a> (见于 11月 21, 2020).</p><ul><li></li></ul>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>卷积神经网络中的参数修改</title>
    <link href="/2023/02/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%AA%97%E5%8F%A3%EF%BC%8Cpadding%E7%AD%89%E7%9A%84%E8%B0%83%E6%95%B4/"/>
    <url>/2023/02/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%AA%97%E5%8F%A3%EF%BC%8Cpadding%E7%AD%89%E7%9A%84%E8%B0%83%E6%95%B4/</url>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络中的卷积窗口padding等的调整"><a class="markdownIt-Anchor" href="#卷积神经网络中的卷积窗口padding等的调整"></a> 卷积神经网络中的卷积窗口，padding等的调整</h1><h2 id="深度学习的通道到底是什么有什么用小白可看"><a class="markdownIt-Anchor" href="#深度学习的通道到底是什么有什么用小白可看"></a> 深度学习的通道到底是什么？有什么用？（小白可看）</h2><h3 id="1什么是通道"><a class="markdownIt-Anchor" href="#1什么是通道"></a> 1.什么是通道？</h3><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaXR0bGViYWkx,size_16,color_FFFFFF,t_70.png" alt="深度学习的通道到底是什么？有什么用？（小白可看）" /> 通道在我看来可以简单理解为图像的深度。通过阅读一些帖子，我觉得有两个总结对理解通道很有帮助，这里在下面介绍一下。</p><p>计算机在存储图片时是以数字矩阵的形式存储，例如我们最常见的彩色图片，RNG格式，它包含红，黄，蓝三个通道，而灰色图片只有一个通道。</p><h3 id="1输入通道数等于卷积核通道个数"><a class="markdownIt-Anchor" href="#1输入通道数等于卷积核通道个数"></a> <strong>1.输入通道数等于卷积核通道个数</strong></h3><p>例如当我们输入的图片为三通道时，那么卷积核也会有三个通道，就像上述图片，最左边的三个矩阵是一个图片的三个通道（因为计算机上是以数字矩阵存储），与这张图片相乘的是一个1*1的三通道卷积核。</p><p>为了让图像的三个通道和卷积核分别进行点积并相加得到一个矩阵，即一个特征图，所以卷积核的通道也要有三个，为了和图像的每个通道都进行运算。</p><h3 id="2卷积核个数-等于-输出通道个数"><a class="markdownIt-Anchor" href="#2卷积核个数-等于-输出通道个数"></a> <strong>2.卷积核个数 等于 输出通道个数</strong></h3><p>卷积核的数量决定输出的通道数，比如说现在有一张像素为16<em>16的三通道图片（16</em>16<em>3），其实这张照片就由三个16</em>16的矩阵组成，如果这时我们有256个3<em>3</em>3的卷积核，其实就是每个卷积核由3个3<em>3的矩阵，有256个这样的卷积核。那么这张16</em>16<em>3的图片要和256个3</em>3*3的卷积核都进行点积并相加得出特征图，即得到的特征图有256个，即卷积核的个数，也是输出通道的个数。</p><h3 id="3为什么要增加通道"><a class="markdownIt-Anchor" href="#3为什么要增加通道"></a> 3.为什么要增加通道</h3><p>在一张照片中可能有很多信息，比如人，动物….，我们人眼可以一下子就分辨出来，但是计算机不可以，他要进行特征提取，也是卷积的第一个操作。</p><p>我们增加通道数就代表着增加特征，而造成通道数增加的操作其实就是卷积核的增加，不同的卷积核可以提取到不同特征，比如说平滑卷积核，它可以让整个图像更加平滑清晰，还比如增加水平边界过滤器，垂直边界过滤器（本质都是卷积核），让图像的矩阵和卷积核进行点积相加，得到不同的矩阵，即不同的特征图，这些特征图越多，越利于计算机学习，这将教会计算机识别特征。</p><p>因为不同的卷积核可以分辨出不同的特征，所以增加卷积核的个数很必要，计算机通过利用这些特征图，来最终得到结论，分辨出图像的事物到底是什么。</p><h3 id="4补充"><a class="markdownIt-Anchor" href="#4补充"></a> 4.补充</h3><p>池化操作是为了在降低像素的同时保存重要信息，而降低像素也十分必要，要通过降低像素来减少计算机的内存消耗。</p><p>网络中常常增加通道同时降低像素，但如果不断降低像素而不增加通道，那么图片通过激活函数等操作后很可能会遗漏重要信息，那么这个网络的训练效果可能就会不理想。</p><blockquote><p>Original: <a href="https://blog.csdn.net/qq_54641516/article/details/127079382">https://blog.csdn.net/qq_54641516/article/details/127079382</a><br />Author: -day day up-<br />Title: 深度学习的通道到底是什么？有什么用？（小白可看）</p></blockquote><blockquote><p><mark>总结而言，卷积核的通道的个数应该与输入的图像的通道个数相同，而一副图像经过卷积操作之后的输出的通道个数应该就是卷积核的个数，即一个卷积核与一副图像卷积操作之后，输出的矩阵的个数就是卷积核的个数，同时需要注意的就是，如果一副图像的输入是多通道的即比如一副图像输入是3通道的，则对应的卷积核应该也是一个3通道的卷积核。</mark></p></blockquote><h2 id="卷积神经网络中的各个参数的调节关系"><a class="markdownIt-Anchor" href="#卷积神经网络中的各个参数的调节关系"></a> 卷积神经网络中的各个参数的调节关系</h2><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230205100827661.png" alt="image-20230205100827661" /></p><p>对于经典的卷积神经网络，比如LeNet而言，其结构如上图所示，在pytorch 中对于卷积层，<code>nn.Conv2d(in_channels=1,out_channels=20,kernel_size=3,padding=1)</code>其中in_channels表示卷积层的输入图像的通道个数，out_channels表示图像经过这个卷积层之后的输出的通道的个数。kernel_size表示这个卷积层的用于卷积的矩阵的大小。在这个例子中</p><p><code>in_channels=12,out_channels=20,kernel_size=3</code>表示输入图像的通道为1，输出通道的个数为20（即表示，卷积核的个数有20个，在现有的神经网络框架中，这些卷积核中的每一个数字都是神经网络自己学习的参数）。</p><p>对于一个卷积层而言，一副图像或者一些数据经过这个卷积层之后的输出是什么样的，应该通过以下两个公式来计算:</p><p>​<img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230205101826734.png" alt="image-20230205101826734" /></p><p>上面的两个公式中的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">H_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示数据经过这个卷积层之后的高度和宽度。在上式中假设输入图像为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>12</mn><mo>×</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">12\times12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">2</span></span></span></span>则计算公式为：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230709155750569.png" alt="" /></p><p>即如果想要输入和输出图像的高度相同，则<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">H_{out}=12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">2</span></span></span></span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>=</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">H_{in}=12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">2</span></span></span></span> kernel_size=3,dilation=1(表示，空洞卷积即卷积核并不是连在一起的矩阵，这个变量的默认值是1，表示不采用空洞卷积。)则上式为：</p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>12</mn><mo>=</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mrow><mn>12</mn><mo>+</mo><mn>2</mn><mo>×</mo><mi>p</mi><mi>a</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>−</mo><mn>1</mn><mo>×</mo><mo stretchy="false">(</mo><mn>3</mn><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mrow><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi></mrow><mo stretchy="false">)</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">12=(({12+2\times padding - 1\times (3-1)-1})/{stride})+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mopen">(</span><span class="mord"><span class="mord">1</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">p</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">1</span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord"><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">i</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 可以得到想要padding和stride的值都比较合理则，padding=1，stride=1（通常来说是先确定stride，stride通常来说等于1，当然可以=2，=3（刚好等于此处的卷积核的宽度，此时通常来说stride就是最大了，因为再大的话就会漏掉信息）等）</p><p>同理宽度的计算也一样。</p><p>所以如果输入的图像为1@12X12即1通道，12X12的图像，则经过上面的第一个卷积层之后的输出为：20@12X12</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230205103414848.png" alt="image-20230205103414848" /></p><p>到池化这里，由于参数设置为每两个值中取一个最大值，同时步长为2，则经过池化之后数据变为：20@6X6，再到下一个卷积层，由于padding和stride的设置，图像经过这一层的卷积层之后仍然不改变图像的高和宽。只是通道数变多了。</p><p>图像经过这个卷积层之后的输出为40@6X6，再经过池化之后为40@3X3通常来说到3X3之后就不会再继续进行卷积核池化不过想要进行卷积的话应该应可以。</p><p>下一步的话就是需要计算展平之后的大小。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230205104101243.png" alt="image-20230205104101243" /></p><p>由于上面的输出可以知道，经过最后的池化层之后的数据为40@3X3即40X3X3=360.所以应该在线性层的输入中填360，至于线性层的输出，由于最终需要的输出的大小为225（15X15）在这里可以先将输入扩大到500，再慢慢缩小到225也可以直接缩小到225.如：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/image-20230205104629678.png" alt="image-20230205104629678" /></p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>什么是迁移学习 (Transfer Learning)</title>
    <link href="/2023/02/07/%E4%BB%80%E4%B9%88%E6%98%AF%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%20(Transfer%20Learning)/"/>
    <url>/2023/02/07/%E4%BB%80%E4%B9%88%E6%98%AF%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%20(Transfer%20Learning)/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是迁移学习-transfer-learning"><a class="markdownIt-Anchor" href="#什么是迁移学习-transfer-learning"></a> 什么是迁移学习 (Transfer Learning)</h1><p><a href="https://www.zhihu.com/question/41979241/answer/123545914">什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？ - 知乎 (zhihu.com)</a></p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/1676377203320.png" alt="1676377203320" /></p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何优化神经网络？（加快训练速度，提高准确度）</title>
    <link href="/2022/06/08/%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9F%EF%BC%88%E5%8A%A0%E5%BF%AB%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%EF%BC%8C%E6%8F%90%E9%AB%98%E5%87%86%E7%A1%AE%E5%BA%A6%EF%BC%89/"/>
    <url>/2022/06/08/%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9F%EF%BC%88%E5%8A%A0%E5%BF%AB%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%EF%BC%8C%E6%8F%90%E9%AB%98%E5%87%86%E7%A1%AE%E5%BA%A6%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="如何优化神经网络加快训练速度提高准确度"><a class="markdownIt-Anchor" href="#如何优化神经网络加快训练速度提高准确度"></a> 如何优化神经网络？（加快训练速度，提高准确度）</h1><h3 id="有钱"><a class="markdownIt-Anchor" href="#有钱"></a> 有钱：</h3><p>Auto ML自动调参（20美元/小时，听说这个调参似乎挺耗时的…）</p><h3 id="人工调参"><a class="markdownIt-Anchor" href="#人工调参"></a> 人工调参：</h3><p>1）<strong>数据输入</strong>：</p><p>最优加速性能不仅依赖于高速的计算硬件，也要求有一个高效的<strong>数据输入</strong>管道。要解决I/O传输问题，有几个方面：SSD、缓冲池、数据读入及处理和模型计算并行起来</p><p>2）<strong>大batch size</strong>：</p><p>为了充分利用大规模集群算力以达到提升训练速度的目的，人们不断的提升batch size大小，这是因为更大的batch size允许我们在扩展GPU数量的同时不降低每个GPU的计算负载。然而，过度增大batch size会带来明显的精度损失！这是因为在大batch size（相对于训练样本数）情况下，样本随机性降低，梯度下降方向趋于稳定，训练就由SGD向GD趋近，这导致模型更容易收敛于初始点附近的某个局部最优解，从而抵消了计算力增加带来的好处。</p><p>解决方向：Learning Rate自适应</p><p>3）<strong>防止模型过拟合</strong>:</p><p>a. 添加<strong>Batch normalization</strong>层：用以规范特征的分布，使输出的特征图分布更加均匀。这个效果一般不错。</p><p>b. 对参数做<strong>正则化</strong>：其他防止模型过拟合的策略比如对参数做正则化，包括weight, bias, BN beta和gamma，这些参数占模型所有参数的比例可能很小，比如在AlexNet模型，它们仅占总参数量的0.02%，对这些参数进行正则化会<strong>增加计算量，还会让模型损失一些灵活性。</strong></p><p>c. <strong>dropout</strong>：dropout可阻碍网络学习仅存在于训练集中局部的“额外规律”，这个效果比较强，但如果加的dropout层过多，比如每层后面都加一个dropout，<strong>阻碍网络学习规律的强度也会增加。</strong></p><p>4）<strong>Inception 结构</strong></p><p>Inception 结构中嵌入了多尺度信息，聚合多种不同感受野上的特征来获得性能增益。参考ResNet变形，GoogLeNet，SENet等。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/v2-0631e3cd60f4b1fa56b6b236dd9af84f_1440w.webp" alt="img" /></p><p>Inception结构</p><p>5）权重初始化</p><p>权重初始化决定了网络从什么位置开始训练，良好的起始位置不仅可以减少训练耗时，也可以使模型的训练更加稳定，并且可以避开很多训练上的问题。比如“dying ReLU”问题，Relu可能会使节点无法被激活，那么就需要对w权重进行合理的初始化；还可以使用leaky relu函数解决。</p><p>6）超参调优</p><ul><li>参数步长由粗到细：调优参数值先以较大步长进行划分，可以减少参数组合数量，当确定大的最优范围之后再逐渐细化调整，例如在调整学习速率时，<strong>采取较大步长测试发现</strong>：学习率lr较大时，收敛速度前期快、后期平缓，lr较小时，前期平缓、后期较快，根据这个规律继续做细微调整，最终得到多个不同区间的最佳学习速率；</li><li>低精度调参：在低精度训练过程中，遇到的最大的一个问题就是精度丢失的问题，通过分析相关数据，放大低精度表示边缘数值，<strong>保证参数的有效性是回归高精度计算的重要方法</strong>；</li><li>初始化数据的调参：随着网络层数的增多，由于激活函数的非线性，初始化参数使得模型变得不容易收敛，可以像VGGNet那样通过首<strong>先训练一个浅层的网络，再通过浅层网络的参数递进初始化深层网络参数</strong>，也可以根据输入输出通道数的范围来初始化初始值，一般以输入通道数较为常见；对于全连接网络层则采用高斯分布即可；对于shortcut的batch norm，参数gamma初始化为零。</li></ul><p>还有其他细节，比如每一个epoch就shuffle训练数据等，想要优化神经网络，加快训练速度，提高准确度，更重要的是创造<strong>更好的网络结构，更有优的公式。</strong></p><p><strong>参考资料：</strong></p><p>【1】机器之心：<a href="https://zhuanlan.zhihu.com/p/40993775">https://zhuanlan.zhihu.com/p/40993775</a></p><p>【2】YJango的前馈神经网络：<a href="https://zhuanlan.zhihu.com/p/27854076">https://zhuanlan.zhihu.com/p/27854076</a></p><p>【3】CS231n Convolutional Neural Networks for Visual Recognition：<a href="https://link.zhihu.com/?target=http%3A//cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></p><ul><li></li></ul>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>卷积神经网络设计入门</title>
    <link href="/2022/03/08/%E5%85%A5%E9%97%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1/"/>
    <url>/2022/03/08/%E5%85%A5%E9%97%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络设计入门"><a class="markdownIt-Anchor" href="#卷积神经网络设计入门"></a> 卷积神经网络设计入门</h1><blockquote><p>这篇文章可以作为一个设计指南，为特定分类任务的 CNN 设计提供指导。作者围绕准确率、速度、内存消耗三个指标的权衡，从网络类型、架构设计、数据处理和迁移学习等方面介绍了 CNN 设计过程中使用的方法。</p></blockquote><p>你想开始做图像分类，但是无从着手。应该使用哪个预训练网络？如何修改网络以使其满足需求？你的网络应该包含 20 层还是 100 层？哪些是最快的、最准确的？这些是你为图像分类选择最好的 CNN 时会遇到的众多问题。</p><p>当选择 CNN 来进行图像分类时，有 3 个非常主要的指标需要去优化：准确率、速度和内存消耗。在这些指标上的性能取决于你所选择的 CNN 以及对它所做的任何修改。不同的网络（如 VGG、Inception 以及 ResNet 等）在这些指标上有不同的权衡。此外，你还可以修改这些网络结构，例如通过削减某些层、增加某些层、在网络内部使用扩张卷积，或者不同的网络训练技巧。</p><p>这篇文章可以作为一个设计指南，为特定的分类任务的 CNN 设计提供指导。尤其是，我们会聚焦在 3 个主要指标上：准确率、速度和内存消耗。我们会研究很多不同的分类 CNN，并探索它们在这 3 个指标方面对应的属性。我们还会研究对这些基本 CNN 可能做出的修改，以及这些修改会怎样影响这些指标。最后，我们会研究如何为特定图像分类任务最优地设计 CNN。</p><p><strong>网络类型</strong></p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/640" alt="图片" /></p><p>在网络类型和这 3 个指标上有着明确的权衡。首先，你肯定会希望使用 Inception 或者 ResNet 类型的设计。它们比 VGGNet 和 AlexNet 更新，而且在速度和准确率之间提供了更好选择的权衡（正如你在上图中看到的）。斯坦福大学的 Justin Johnson 对其中的一部分做了很棒的基准测试（<a href="https://github.com/jcjohnson/cnn-benchmarks%EF%BC%89%E3%80%82">https://github.com/jcjohnson/cnn-benchmarks）。</a></p><p>Inception 和 ResNet 的选择确实是速度和准确率的权衡：要准确率，用超深层的 ResNet；要速度，用 Inception。</p><p><strong>使用巧妙的卷积设计来减少运行时间和内存消耗</strong></p><p>对 CNN 一般设计的最新进展已经提出了一些非常棒的可选择方法，它们能够在没有太多的准确率损失的情况下加速 CNN 的运行，并减少内存消耗。所有的这些方法都可以很容易地集成在上述的任何一类卷积神经网络中。</p><ul><li>MobileNets（<a href="https://arxiv.org/pdf/1801.04381.pdf%EF%BC%89%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%88%86%E7%A6%BB%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%9D%A5%E6%9E%81%E5%A4%A7%E5%9C%B0%E5%87%8F%E5%B0%91%E8%BF%90%E7%AE%97%E5%92%8C%E5%86%85%E5%AD%98%E7%9A%84%E6%B6%88%E8%80%97%EF%BC%8C%E5%90%8C%E6%97%B6%E4%BB%85%E7%89%BA%E7%89%B2">https://arxiv.org/pdf/1801.04381.pdf）使用深度分离的卷积来极大地减少运算和内存的消耗，同时仅牺牲</a> 1% 到 5% 的准确率，准确率的牺牲程度取决于你想要获得的计算节约。</li><li>XNOR-Net（<a href="https://arxiv.org/pdf/1603.05279.pdf%EF%BC%89%E4%BD%BF%E7%94%A8%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8D%B7%E7%A7%AF%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E5%8F%AA%E6%B6%89%E5%8F%8A%E4%B8%A4%E4%B8%AA%E5%8F%AF%E8%83%BD%E7%9A%84%E6%95%B0%E5%80%BC%EF%BC%9A0">https://arxiv.org/pdf/1603.05279.pdf）使用二进制卷积，也就是说，卷积运算只涉及两个可能的数值：0</a> 或者 1。通过这种设计，网络可以具有较高程度的稀疏性，易于被压缩而不消耗太多内存。</li><li>ShuffleNet（<a href="https://arxiv.org/pdf/1707.01083.pdf%EF%BC%89%E4%BD%BF%E7%94%A8%E7%82%B9%E7%BB%84%E5%8D%B7%E7%A7%AF%E5%92%8C%E9%80%9A%E9%81%93%E9%9A%8F%E6%9C%BA%E5%8C%96%E6%9D%A5%E6%9E%81%E5%A4%A7%E5%9C%B0%E5%87%8F%E5%B0%91%E8%AE%A1%E7%AE%97%E4%BB%A3%E4%BB%B7%EF%BC%8C%E5%90%8C%E6%97%B6%E8%BF%98%E8%83%BD%E7%BB%B4%E6%8C%81%E6%AF%94">https://arxiv.org/pdf/1707.01083.pdf）使用点组卷积和通道随机化来极大地减少计算代价，同时还能维持比</a> MobileNets 高的准确率。事实上，它们可以在超过 10 倍的运算速度下达到早期最先进的分类 CNN 的准确率。</li><li>Network Pruning（<a href="https://arxiv.org/pdf/1605.06431.pdf%EF%BC%89%E6%98%AF%E4%B8%BA%E4%BA%86%E5%87%8F%E5%B0%91%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4%E5%92%8C%E5%86%85%E5%AD%98%E6%B6%88%E8%80%97%E8%80%8C%E5%88%A0%E9%99%A4">https://arxiv.org/pdf/1605.06431.pdf）是为了减少运行时间和内存消耗而删除</a> CNN 的部分权重的技术，而且有希望不降低准确率。为了保持准确率，被删除的部分应该对最终结果没有大的影响。链接中的论文展示了使用 ResNets 可以轻易地做到这一点。</li></ul><p><strong>网络深度</strong></p><p>这个比较容易：通常增加更多地层会提升准确率，同时会牺牲一些速度和内存。然而，我们已经意识到的是这种权衡受制于边际效应，也就是说，我们增加的层越多，通过增加每一层而带来的准确率提升将越少。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/640" alt="图片" /></p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/640" alt="图片" /></p><p><strong>激活函数</strong></p><p>关于激活函数，最近有很多争议。然而，很好的一个经验法则就是从 ReLU 开始。使用 ReLU 通常会在开始的时立即得到一些好的结果。不像 ELU、PReLU 或者 LeakyReLU 一样还需要一些繁琐的调整。当你确定你的设计使用 ReLU 能够达到不错的效果，那你就可以调整其它的部分，并调整它们的参数，以尝试对准确率做最后的提升。</p><p><strong>卷积核大小</strong></p><p>你也许认为使用更大的卷积核总会导致最高的准确率，同时还会损失速度和内存。然而，情况并不总是如此，因为研究中多次发现使用较大的卷积核会使得网络难以发散。使用更小的核（例如 3×3）会更好一些。ResNet 和 VGGNet 都相当全面的诠释了这一点。正如这两篇论文所展示的，你也可以使用 1×1 的核来减少特征的数目。</p><p><strong>空洞卷积</strong></p><p>为了能够使用远离中心的像素，空洞卷积（Dilated Convolution）在卷积核的权重之间使用空格。这使得网络不用增加参数数目就能够指数级地扩展感受野，也就是说根本没有增加内存消耗。已经证明，空洞卷积可以在微小的速度权衡下就能增加网络准确率。</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/640" alt="图片" /></p><p><strong>数据增强</strong></p><p>你应该经常做数据增强。使用更多的数据已经被证明能够持续地增强性能，甚至达到极限（<a href="https://arxiv.org/pdf/1707.02968.pdf%EF%BC%89%E3%80%82%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%EF%BC%8C%E4%BD%A0%E5%8F%AF%E4%BB%A5%E5%85%8D%E8%B4%B9%E8%8E%B7%E5%BE%97%E6%9B%B4%E5%A4%9A%E6%95%B0%E6%8D%AE%E3%80%82%E5%A2%9E%E5%BC%BA%E7%B1%BB%E5%9E%8B%E5%8F%96%E5%86%B3%E4%BA%8E%E4%BD%A0%E7%9A%84%E5%BA%94%E7%94%A8%E3%80%82%E6%AF%94%E5%A6%82%EF%BC%8C%E5%A6%82%E6%9E%9C%E4%BD%A0%E5%81%9A%E7%9A%84%E6%98%AF%E6%97%A0%E4%BA%BA%E8%BD%A6%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%8C%E4%BD%A0%E5%8F%AF%E8%83%BD%E4%BC%9A%E9%81%87%E5%88%B0%E8%B7%AF%E4%B8%8A%E7%9A%84%E8%BD%A6%E3%80%81%E6%A0%91%E4%BB%A5%E5%8F%8A%E5%BB%BA%E7%AD%91%E7%89%A9%EF%BC%8C%E6%89%80%E4%BB%A5%EF%BC%8C%E5%B0%86%E4%BD%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%9E%82%E7%9B%B4%E7%BF%BB%E8%BD%AC%E6%98%AF%E6%B2%A1%E6%9C%89%E6%84%8F%E4%B9%89%E7%9A%84%E3%80%82%E7%84%B6%E8%80%8C%EF%BC%8C%E4%BD%A0%E4%B8%80%E5%AE%9A%E4%BC%9A%E9%81%87%E5%88%B0%E7%94%B1%E4%BA%8E%E5%A4%A9%E6%B0%94%E5%8F%98%E5%8C%96%E6%88%96%E8%80%85%E5%9C%BA%E6%99%AF%E5%8F%98%E5%8C%96%E8%80%8C%E5%BC%95%E8%B5%B7%E7%9A%84%E5%85%89%E7%BA%BF%E6%94%B9%E5%8F%98%EF%BC%8C%E9%80%9A%E8%BF%87%E6%94%B9%E5%8F%98%E5%85%89%E7%BA%BF%E5%92%8C%E6%B0%B4%E5%B9%B3%E7%BF%BB%E8%BD%AC%E6%9D%A5%E5%A2%9E%E5%BC%BA%E6%95%B0%E6%8D%AE%E6%98%AF%E6%9C%89%E6%84%8F%E4%B9%89%E7%9A%84%E3%80%82%E5%8F%AF%E4%BB%A5%E7%9C%8B%E4%B8%80%E4%B8%8B%E8%BF%99%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%BA%93%EF%BC%88https://github.com/aleju/imgaug%EF%BC%89">https://arxiv.org/pdf/1707.02968.pdf）。使用数据增强，你可以免费获得更多数据。增强类型取决于你的应用。比如，如果你做的是无人车的应用，你可能会遇到路上的车、树以及建筑物，所以，将你的图像垂直翻转是没有意义的。然而，你一定会遇到由于天气变化或者场景变化而引起的光线改变，通过改变光线和水平翻转来增强数据是有意义的。可以看一下这个数据增强库（https://github.com/aleju/imgaug）</a></p><p><strong>训练优化器</strong></p><p>当你最终要训练网络的时候，有几种可以选择的优化算法。很多人说 SGD 在准确率方面会得到最好的结果，在我的经验看来，这是正确的。然而，调整学习率设置和参数是枯燥的，也是具有挑战性的。另一方面，虽然使用自适应的学习率（例如 Adam,、Adagrad 或者 Adadelta）比较容易，也比较快速，但是你可能得不到和 SGD 一样的最优准确率。</p><p>最好就是让优化器遵循和激活函数一样的「风格」：先使用最容易的，看看它是否奏效，然后使用更复杂的来调节和优化。我个人推荐以 Adam 作为开始，因为根据我的经验，它最容易使用：设置一个不太高的学习率，一般默认 0.0001，然后你通常会得到一些非常好的结果。随后你可以从零开始使用 SGD，或者甚至以 Adam 作为开始，然后使用 SGD 精调。事实上，<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650735242&amp;idx=5&amp;sn=11de56fd5a6f40ceaaebc3ca7c519b24&amp;chksm=871ac6f4b06d4fe28e74bc0ef95bdfcec3321241e2728f5e7adf0f787394b6c7c262a03c747e&amp;scene=21#wechat_redirect">这篇文章</a>发现使用 Adam，中间换到 SGD，能够以最容易的方式达到最好的准确率。看一下论文中的这张图：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/640" alt="图片" /></p><p><strong>类别均衡</strong></p><p>很多情况下你会遇到不均衡数据，尤其是在现实应用中。举一个现实中的简单例子：由于安检原因，你在训练深度网络来预测输入视频中的某人是否持有杀伤性武器。但是在你的训练数据中，你只有 50 个视频中的人持有武器，而 1000 个视频中的人是没有持有武器的！如果你立即使用这些数据训练你的网络，你的模型一定会以很高的偏差偏向于预测某人未持有武器。</p><p>可以用以下的方法来解决类别不均衡问题:</p><ul><li>在损失函数中使用类别权重。本质上，样本数量不足的类别在损失函数中接受较高的权重，这样的话特定类别的误分类会在损失函数中导致较高的误差。</li><li>过采样：对训练样本中数量不足的类别进行重复采样，这样有助于样本分布的均衡化。当可用数据较少的时候这个方法最能奏效。</li><li>降采样：你也可以简单地跳过包含过多样本的类别中的一些训练样本。当可用数据非常多的时候，这个方法最奏效。</li><li>数据增强：对少数类别的数据进行数据扩增。</li></ul><p><strong>优化你的迁移学习</strong></p><p>对大多数应用而言，使用迁移学习要比从零开始训练网络更加合适。然而，需要选择的是：你要舍弃哪些层，保留哪些层。这非常依赖于你的数据。你的数据和预训练网络（通常是在 ImageNet 上训练）所用的数据越相似，你需要重新训练的层就越少，反之亦然。例如，假设你要训练网络来区分一张图片是否包含葡萄，所以你会有大量的包含葡萄和不包含葡萄的图像。这些图像和 ImageNet 中使用的图像相当相似，所以你仅仅需要重新训练最后几层，或许只需要重新训练全连接层。然而，倘若你要分类的是一幅外太空的图像是否包含一颗行星能够，这种数据和 ImageNet 中的数据大有不同，所以你还需要重新训练后边的卷积层。简而言之，应该遵循以下的原则：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/640" alt="图片" /></p><p><strong>总结</strong></p><p>本文给出了用于图像分类应用而设计 CNN 的全面指南。希望你能够愉快地阅读本文，并学到一些新的、有用的东西。</p><p><em>原文链接：</em></p><p><em><a href="https://hackernoon.com/a-comprehensive-design-guide-for-image-classification-cnns-46091260fb92">https://hackernoon.com/a-comprehensive-design-guide-for-image-classification-cnns-46091260fb92</a></em></p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>python中分割多个空格分割的字符串</title>
    <link href="/2022/02/08/python%E4%B8%AD%E5%88%86%E5%89%B2%E5%A4%9A%E4%B8%AA%E7%A9%BA%E6%A0%BC%E5%88%86%E5%89%B2%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
    <url>/2022/02/08/python%E4%B8%AD%E5%88%86%E5%89%B2%E5%A4%9A%E4%B8%AA%E7%A9%BA%E6%A0%BC%E5%88%86%E5%89%B2%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
    
    <content type="html"><![CDATA[<h1 id="python中分割多个空格分割的字符串"><a class="markdownIt-Anchor" href="#python中分割多个空格分割的字符串"></a> python中分割多个空格分割的字符串</h1><p>做法： 1.str.<a href="https://so.csdn.net/so/search?q=split&amp;spm=1001.2101.3001.7020">split</a>()</p><p>2.filter(None,str.split(&quot; &quot;))</p><p>直接用str.split(&quot; &quot;)是不行的，他只会分割一个空格，如下</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">str = <span class="hljs-string">&quot;aa   bbbbb         ccc  d&quot;</span><br>str_list = str<span class="hljs-selector-class">.split</span>(<span class="hljs-string">&quot; &quot;</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(str_list)</span></span><br><span class="hljs-number">123</span><br></code></pre></td></tr></table></figure><p>结果： <img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/2020081017370873.png" alt="在这里插入图片描述" /></p><p>第一种做法： 实际上，split()函数默认可以按空格分割，并且把结果中的空字符串删除掉，留下有用信息</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">str = <span class="hljs-string">&quot;aa   bbbbb         ccc  d&quot;</span><br>str_list = str<span class="hljs-selector-class">.split</span>()<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(str_list)</span></span><br><span class="hljs-number">123</span><br></code></pre></td></tr></table></figure><p>结果：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/20200810173758370.png" alt="在这里插入图片描述" /></p><p>第二种做法： 可以用filter函数对split（“ ”）进行过滤 filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表,filter(function, iterable) Python2.x 中返回的是过滤后的列表, 而 Python3 中返回到是一个 filter 类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;aa   bbbbb         ccc  d&quot;</span><br>str_list = <span class="hljs-built_in">filter</span>(<span class="hljs-literal">None</span>,<span class="hljs-built_in">str</span>.split(<span class="hljs-string">&quot; &quot;</span>))<br><span class="hljs-built_in">print</span>(str_list)<br><span class="hljs-number">123</span><br></code></pre></td></tr></table></figure><p>结果：</p><p><img src="https://cdn.jsdelivr.net/gh/csGituser/blogImages/Images/20200810174349129.png" alt="在这里插入图片描述" /></p><p>filter第一个参数是None的时候，返回第二个参数中非空的值。</p><p>可能第一种做法更方便一些。</p>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
